---------------------------------------------------------------------------------------------------------
INTRODUCTION::
---------------------------------------------------------------------------------------------------------

NOTE: From Nov 2022, exam pattern is changing for Solution Architect - Professional.


=========================================================================================================
AWS ACCOUNTS: 
=========================================================================================================

Introduction: 

    AWS Account is container for identity (users) and resources. 

    Account Root user - Create account with unique email address and using credit card. It has full access control to aws resources. 

    By default all access to aws account & resources are DENIED except for Account root user. It requires to explicit ALLOW to access resources. 


Multi Factor Authentication: 

    Factors - It is different pieces of evidences which prove identity.
    
    Multi Factors - Multiple factors to prove idendity. 
    
    Types of factors: 
        Knowledge   -   username & password 
        Possession  -   card # , MFA device. 
        Inherent    -   fingerprint, iris 
        Location    -   location of user. 

    More factors means more security. 

    In AWS, based on username and password generates the KEY + other information in QR format which can be scanned to generate the verification code. 


Securing an AWS Account: 

    Using MFA: 

        Select root account -> Security Credentials. 


Creating budget: 

    Cost explorer provides reporting, analytics and visualization capabilities that can use to track and manage aws costs. 

            root account -> Billing dashboard -> cost management -> cost explorer 

    AWS budget is used to set custom budget to alert when threshold is reached. It requires cost explorer to be enabled to work. 

            root account -> Billing dashboard -> cost management -> budget 

            Create Budget: 
                    1. Select budget limit, budget name 
                    2. Select email recepients. 
                    3. Can select SNS target. 


Creating production account: 

    Create a brand new account as production with MFA feature and enable the budget alert. 


Adding an IAM Admin: 

    IAM -> Users -> Attach permission directly.

    IAM -> Users -> Group -> In group, attach permission

    https://meeran-general.signin.aws.amazon.com/console
    https://meeran-production.signin.aws.amazon.com/console


IAM Access Keys:     

    username and password is single attached to user. 

    access key can be multiple and can be enable, disable and delete. 

    aws configure --profile profile-name 


Access Advisor: 
    Users should have least previlege. 
    Allows unused permission to be identified. 
    See permissions granted and when last accessed.
    This is at user, group and role level.
    IAM -> User/group/role -> Access Advisor tab.

Access Analyzer: 
    IAM -> Access Report -> Access Analyzer -> Create analyzer at organization or account level.
    Analyze resource permissions.
    It helps to identify potential security risks in AWS environment by analyzing resource based policies applies to resources 

IAM Role Vs Resource based policy: 
    In IAM Role, Principal lose it own permission and takeup the assume role permission. Ex: Account A scans dynamodb and dump in s3 another account. 
    In Resource based policy, Principal doesnot lose its own permission.

=========================================================================================================
ADVANCED PERMISSIONS AND ACCOUNTS: 
=========================================================================================================

AWS ORGANIZATIONS: 

    Each organization requires multiple accounts and managing separately is tedious if account increases.
    
    With AWS Organization, accounts are managed.

    Management account is from which account organization is created. 

    Master or Management or payer account. Using management account, can invite other accounts to join. Once joined, standard account become memeber account of organization.

    Using AWS organization, can create new account directly or invite existing account. 

    Consolidated billing features. 

    Service Control Policy 

    General practice is to create separate accounts for payments (management), user account (integrate with federation iddendity). From user account, do switch role to assume the role in other accounts. 

    All accounts in organization can receive hourly cost benefit of Reserved Instance that are purchased by any other account. Management account can turn off RI discount and savings plan discount sharing for any accounts including management account. 
    To share both accounts must have sharing turned on. 

    Account -> Billing preference -> Biling preferences -> RI & Savings plan discount sharing. 

    CONSOLE: 

        Create Organization:  

            account -> organization -> create organization -> Automatically added as management account. 

        Invite the account: 

            account -> organization -> Add an AWS account -> Send Invite to existing account. 

        Accept the invitation: 

            account -> organization -> Invitations -> Accept Invitations 

        Create Account: 

            account -> organization -> create account -> unique email + name + rolename 

        Create Organization Unit (OU): 

            account -> organization -> Root -> Create organization 


    ROLE SWITCH: 

        For account created using organization, ROLE (OrganizationAccountAccessRole) is created automatically and can be use switch role.

        For invited account, requires to create the role. 

        CREATE ROLE IN INVITED ACCOUNT FOR MANAGEMENT ACCOUNT TO USE FOR ROLE SWITCH: 

            IAM -> Role -> AWS Account -> Management account nbr -> Attach policy 

        TO SWITCH ROLE: 
            In AWS Organization, it helps administrator to easily manage accounts. 

            From management account -> account -> Switch Role -> 
    
    
    SERVICE CONTROL POLICY: 

        It is a JSON policy and can be attached at Root level, OU level or account level. 

        If attached at root level, then affect all OU and accounts. If at OU level, affect the accounts under the OU.

        NOTE: Management account cannot be affect by this SCP at any level. 

        SCP doesnot grant any permission but it is account level permission boundaries. 

        Account root user has full access in account but indirectly can be restricted by using SCP.

        SCP doesnot affect service linked roles. 

        aws:TagKeys - used to check the key value of tags. 

        Access can be provided by 2 ways, 
            1. Explicit allow all (default policy) and explicit deny few services.
            2. Delete default and Explicit allow few services only. (more secure)

            Implicit Deny -> Explicit Allow -> Explicit Deny 

        CONSOLE: 
            1. First to enable the SCP: 
                    Organization -> Policy -> SCP -> ENABLE (Default create with FullAWSAccess policy) and attach to ROOT by default.

    TAG POLICY: 

        Aws Organizations -> Policies -> Tag policies -> Enable 

        It helps to standardize tags across resources in AWS organizations.
        Can define Tag keys and allowed values. 

    AI SERVICES OPT OUT POLICY:

    BACKUP POLICY:

---------------------------------------------------------------------------------------------------------

SECURITY TOKEN SERVICE (STS):

    Whenever role is assumed, it pass to STS:AssumeRole*.

    User -> Assume a Role -> STS -> Check Identity maintained in Trust policy   -> If allowed, then get the permission -> Return access key, secret id, session token & expiration date. 
    
    STS does 2 things, 
            1. Check trust policy is maintained. 
            2. If maintained, get the permission policy and return the access details.

    Identity can be switch role, cross account role and other AWS resources. 

    Temporary credentials valid from 5 mins to 12 hrs. 

    Zone of trust - accounts, organizations that you own.
    Outside zone of trust - 3rd parties. 

    External ID - used to avoid "The confused deputy"

    AssumeRole
    AssumeRolewithSAML
    AssumeRolewithWebIdentity
    GetSessionToken
    GetFederationToken 


REVOKING IAM ROLE TEMPORARY SECURITY CREDENTIALS: 
    
    Temporary credentials cannot be manually cancelled or invalidated and it is valid till expiry time.

    Incase if temporary credentials are compromised, "Revoke session" is used to add a conditional deny policy which DENY any session assumed < NOW. So legitimate users can assume the role again but illegitimate users cant assume again.

    During Revoke session, it will add "AWSRevokeOlderSessions" inline policy to role. 

    CONSOLE: 

        IAM -> Role -> View Role -> Revoke Sessions -> Revoke active sessions. 

                    "Condition": {
                        "DateLessThan": {
                            "aws:TokenIssueTime": "[policy creation time]"
                        }
                    }

---------------------------------------------------------------------------------------------------------

POLICY INTERPRETATION: 

    Implicit Deny -> Explicit Allow -> Explicit Deny 

    {
        Version: "2012-"
        Statement: [{
            Sid: "statement id 1"
            Effect: "allow" or "deny"
            Action: ["s3*", "ec2*"]  or NotAction: []
            Resource: "s3*"
            Condition: {}
        }, {}, {}]
    }

    NOTE: If single statement with DENY effect. It is no useful because by default implicit DENY. So single statement with DENY is used in conjugation with other ALLOW policy.

    Concentrate on NotAction , StringNotEquals conditions.

    "Condition": { "{condition-operator}": {"{condition-key}":"{condition-value}"}}



PERMISSION BOUNDARIES: 

    This is at identity level. It doesnot grant any access. It limit the permission of the identity permission.

    It helps for delegation access. 
    It supported for users and role (not groups) 

    For example: 
        Bob (admin) believes tom and want to give full iam:* access.
        This cause probelms, 
            Tom can change his own IAM permission.
            Create another user with administrator access.

    To solve this problem,
        1. Bob create a user boundary and allow only that user level access ${aws:user}
        2. Create user Tom and assign a permission boundary + policy not to change his identity + cant edit permission boundary. 


PERMISSIONS EVALUATION LOGIC: 

    For same account: 

        Explicit Deny -> SCP -> Resource Policy(Allowed then stops)  -> Permission Boundary -> Session Policy (Assume a Role) -> Identity policy 

    For cross account: 

        Account A (Identity policy) -> Account B (Resource policy)
        Account A to allow access to Account B <> Account B to allow access from Account A 
        Allow <> Allow  then Allow 
        Deny <> Allow   then Deny 
        Allow <> Deny   then Deny 

---------------------------------------------------------------------------------------------------------

CROSS ACCOUNT ACCESS TO S3: 

    ACL: 

        It is legacy method of giving permission and AWS no longer recommend this. 
        Account canonical user id 
        Whoever uploaded (owner) only have permission even bucket owner is different.

        1. Create the bucket in Account B. 
            a. Create the ACL access to allow from A. Use Account A Canonical name.
        2. From Account A, Switch Role to Account B.
        3. Upload the objects in Account B.
        4. Only Account A have access (owner) not Account B (Bucket owner).

        NOTE: y default, "Object ownership" - helps to override this behaviour and allows bucket owner to own the object.

        Objects owned by uploader. 


    BUCKET POLICY:

        1. Create a bucket policy.  
                principal = Account A allow to access. 
        2. Attach the policy to Account B
        3. From Account A, able to access Account B. 

        Objects owned by uploader. 


    CROSS ACCOUNT ROLE: 

        Objects owned by bucket owner. 

        1. Create a role with trust policy of cross account and s3 bucket permission.
        2. From Account A, switch role to s3 role.

---------------------------------------------------------------------------------------------------------

AWS RESOURCE ACCESS MANAGER (RAM): 

    Within organization, it helps to share resources at organization, OU and Account level.

    Only RAM supported products can be shared.

    No cost for RAM but cost for underlying resources used.

    It influences changes in traditional architecture.

    NOTE:   Availability zone in 1 account (us-east-1) is not same for another account (us-east-1)
            Availability zone ID (use1-az1) is consistent or same accross the accounts. 

    Owner Account:  Owns the resource and create a share. 
                    Owner retains full permission.
                    Defines the principal (org,ou,acc) to whome to share.
                    If participant inside an ORG, acceptance happen automatically. Any non ORG account, based on invite to accept. 
    
    USE CASE: 

        In organization, owner account create a Subnet & security group and shared with participant accounts.
        Participant accounts can only have Read access to utilize and cannot be modified.
        Only owner account can modify, delete created shared resources.
        If participant account create ec2 instance under shared subnets, ec2 is owned by only participant account not owner account. 

    CONSOLE: 

        RAM -> Settings -> Enable Sharing within organization.

        RAM -> Create resource share -> Name, Resources, Organizations -> Create 

        Resources -> Shared by Me , Resources -> Shared with Me

        NOTE: Name is not tagged in shared resource and participant account can update name. One participant account created resource cannot seen by other participant accounts. 

---------------------------------------------------------------------------------------------------------

AWS IAM IDENTITY CENTER: 

    Successor to AWS single signon.

---------------------------------------------------------------------------------------------------------
SERVICE QUOTAS: 

    Quota is limit on the AWS resources.
    Service normally have per region quota.
    Global services have account level quota.
    Most service quotas can be increased.
    Hard limit cannot be changes: 
        IAM user of 5000 per account.

    https://docs.aws.amazon.com/general/latest/gr/aws-service-information.html

    AWS -> Service Quotas -> AWS Resources.
        Quota Name, Applied Quota value, Aws default quota value, Adjustable
        View quota -> Request Quota Increase 

        View quota -> Monitoring. 
            If threshold is reached, then alarm the quota limit.

    Organization -> Quota Request Template.

    Support -> Ticket to increase quota limit.


=========================================================================================================
ADVANCED IDENTITIES AND FEDERATION: 
=========================================================================================================

CUSTOM IDENTITY BROKER: 

    No SAML is used here.

    User -> Custom identity broker -> Authenticate -> Custom Idp directly call STS Assume role or get federation token to AWS. 

---------------------------------------------------------------------------------------------------------

WEB IDENTITY FEDERATION - WITHOUT COGNITO: 

    Client (mobile or web)  -> Login with google/facebook/openid connect compatible -> returns web identity token -> Assume role with web identity  -> STS -> Temporary credential received.

---------------------------------------------------------------------------------------------------------

SAML2.0 IDENTITY FEDERATION: 

    NOTE: AWS Only understand the AWS credentials to allow access to resources.

    Here using another identity provider to get identity to access AWS resources.

    SAML2 is used in enterprise identity federation.

    SAML2: 
        Security Assertion Markup language
        Many onpremise identity provider uses SAML2 

    use case: 
        If no of user > 5000
        Enterprise identity provider + SAML2 compatible
        If requires existing IAM team to manage with single source of identity truth.

    It uses IAM roles and Temporary security credential valid for 12 hours. 

    ARCHITECTURE for Application: 

        1. Trust relationship establish between Idp and IAM.
        2. Application call Idp which authenticated and return SAML assertion.
        3. Application call STS to get temporary credential.
        4. Application call AWS resources to access.

    ARCHITECTURE for USER LOGIN: 

        1. Trust relationship establish between Idp and SAML/SSO endpoint.
        2. Client call Idp which authenticated and return SAML assertion.
        3. CLient send the SAML assertion to SSO endpoint. 
        4. From SSO, call STS assume role.
        5. SAML Endpoint generate the URL and Client uses to access AWS console.

---------------------------------------------------------------------------------------------------------

AWS SINGLE SIGN ON (SSO): 

    It helps to centrally manage sso access and user permission for accounts in organization.
    
    Requirement is valid AWS organization is created.

    It is extension of AWS organization.

    AWS SSO supports multiple identity source, 
        built in identity stores.
        AWS Managed microsoft AD 
        On premise microsoft AD 
        External SAML2 identity
        AD connector. 

    AWS SSO is recommended. 

    https://jumpcloud.com/blog/aws-iam-vs-aws-sso


    CONSOLE: 

        AWS SSO -> Enable 

        https://d-9067b1117b.awsapps.com/start  - can customize the URL.

        Identity source -> AWS SSO, Active Directory , External Identity Provider.

        Permission sets -> Predefined or custom permission sets. Session duration can select.

        Users & Groups can be created.


        User portal URL: https://d-9067b1117b.awsapps.com/start, Username: sally, One-time password: 

        1. Create a user, group and permission sets.
        2. From aws account, assign user/group to account.
        3. Assign a permission (Permission Set Name is displayed).

        AWS SSO -> Applications -> Can integrate 3rd party applications.

    To enable MFA: 

        SSO -> Settings -> Network & Security -> MFA 

---------------------------------------------------------------------------------------------------------

AWS CONTROL TOWER: 



---------------------------------------------------------------------------------------------------------

AMAZON COGNITO - USER AND IDENTITY POOLS: 

    It provides authentication, authorization and user management for web/mobile applications.

    It consists of user pools and identity pools
        User Pools -    Signin and provide JWT (Json web token) and few AWS services accepts JWT like API gateway. 
                        It supports SAML2 , Facebook, login, Identity federation.
                        Used for signup and signin.

        identity pools -    It provides AWS credentials by exchange JWT.
                            Even user pool , google, saml2 identity used for exchange.
                            Swapping authenticated or unauthenticated identity for credentials.
                            Assume the role on behalf of identity.
                            NOTE: It can deal both authenticated or unauthenticated identity.

    1. User is authenticated with external identity like google, facebook etc.
    2. User pools returns the JWT to web/mobile application.
    3. Web/mobile application send JWT to Identity Pool.
    4. Identity Pool assume the role and generate the temporary credentials.
    5. Identity pool return the temporary credentials to web/mobile application.

    CONSOLE: 

        Cognito -> Create a user pool 
        Cognito -> Identity pool or Federated identities.

---------------------------------------------------------------------------------------------------------

AMAZON WORKSPACE: 

    It is AWS Desktop As A Service.

    It uses directory services (simple AD or AD Connector) for user management and authentication.

    NOTE: Only support AWS Directory services (Simple AD, Microsoft AD & AD Connector)

    It consists of AWS Managed and Customer managed VPC.

    All management related services (AD, Workspace, authentication gateway) in AWS managed VPC.

    From AWS Managed VPC, these services interact with Customer managed VPC through ENI. From there it connect to VPN or Direct connect through Nat gateway and Internet gateway. 

    To function workspace, it requires Directory services and VPC.
    
---------------------------------------------------------------------------------------------------------

AWS MANAGED MICROSOFT AD: 

    Standard    -   support 5000 users and 30000 objects.
    Enterprise  -   Support upto 500,000 objects.

    If require native microsoft authentication, then recommend this.

    Can configure a trust relationship between AWS managed microsoft AD and On prem microsoft AD, so users and groups access resources in either domain.

    Capable of operating independently if link broken between Onprem and AWS.

    Can integrate with MFA (Radius based server).

    Support microsoft schema extension.

    Can establish two way trust between Onprem and AWS managed AD.

---------------------------------------------------------------------------------------------------------

SIMPLE AD: 

    Cannot join with onpremise AD.
    AD compatible managed directory on AWS.
    Doesnot support MFA.
    Simpler and cheaper. 
    Powered by samba4.
    No trust relationship with onprem AD.

---------------------------------------------------------------------------------------------------------

AD CONNECTOR: 

    It appears as native directory.
    It send the request to onprem active directory and does not store any data.

    Useful in proof of concept and no need to keep AD in AWS.

    It consists of below based on compute power,
        Small
        Large

    It placed in 2 subnets with in VPC and requires 1 or more AD .

    It requires working network connectivity between AWS and On prem through Direct connect or VPN connection etc.

    Select only when,
        Proof of concept
        Legal or compliance reason not place in AWS 


=========================================================================================================
NETWORKING AND HYBRID: 
=========================================================================================================

PUBLIC AND PRIVATE AWS SERVICES: 

    Public  - Have public endpoint and accessed using internet.  
    Private - Runs within VPC.

    Public zone     -   act as interface between private and internet. From this zone, aws public service operates like s3 etc. 
    Private Zone    -   VPC 

    From private zone, resources like ec2 uses internet gateway to access internet.

    Private to private connection happen using VPN or direct connect. 

---------------------------------------------------------------------------------------------------------

DHCP IN VPC: 

    Dynamic Host Configuration Protocol - It helps automatic configuration of network resources. It is useful to assign dynamic IP to host generally.

    It consists of Client and Server, based on Layer 2 has MAC address.

    Inorder to work at Layer 3, minimum requires IP address, Subnet masks & Default gateway.

    In VPC,
        DNS Servers, NTP Server, Domain Name, NetBios 

    NOTE: Once DHCP Option sets created, it cannot be modified and immutable.
    
    Option set can be associated with 1 or more vpc. Each VPC have 1 DHCP option set.

    NOTE: Associate VPC with DHCP can happen immediately but to effect changes require "DHCP Renew"

---------------------------------------------------------------------------------------------------------

VPC ROUTER: 

    It helps to route traffic, 

        1. Between subnets.
        2. From external network to VPC
        3. From VPC to external network.
    
    It is highly available and across all AZ. 

    Router is attached to subnet level.Each subnet is associated with route table. 

    Subnet have interface with Router through ENI (Subnet ip address + 1)

    Router is managed by using route table.

    NOTE:   Subnets are associated with only one route table. Same route tables can be associated with many subnets. 

    /32 is higher priority than /16, /8 or /0.

    Route Table: 
        Destination     Target 
        10.0.0.0/16     local           (IP address is VPC CIDR range and mean local VPC)
        0.0.0.0/0       igw             (It matches all IP address and generally use to connect to internet)

    Packets contains source and destination IP address. Destination IP address is compared with destination prefix to route to the target.

---------------------------------------------------------------------------------------------------------

STATELESS VS STATEFUL FIREWALLS: 

    In Layer 4, TCP, uses the ephermal port to initiate the request from client to server in well known port.

    It is bidirectional and request/response process separately. 

    Stateless which doesnot understand the state of connection whether request or response, so requires both inbound and outbound rule. Since client port is ephermal so outbound is allowed always.

    security group is stateful because smart enough to get the response ip from the request details.


    NACL: 

        At subnet level.
        stateless.
        Since ephermal port require to allow all outbound port. 

    NOTE:   Default NACL, allow both inbound and outbound. 
            Custom NACL, denies both inbound and outbound.

        Deny is feature in NACL which helps to block IP ranges.


    SECURITY GROUP: 

        At ec2 level.
        Stateful.
        No need to allow outbound port.

        Major disadvantage is no explicit deny, so used in conjuction with NACL.

        NOTE: Security groups are not attached to instances but attached to ENI (Elastic Network Interfaces).

        It gives additional feature of logical reference of another security group. For example: In backend app-security-group, include the source as frontend-security-group.

        Also can self reference security group: 
                If self reference the security group and attach to multiple instances. It allow communication between them.

---------------------------------------------------------------------------------------------------------

BORDER GATEWAY PROTOCAL: 

    AS - Autonomous System
    ASN - Autonomous System Number
    It helps to find the shortest route to reach the destination.
    Each network has AS (Autonomous System) and ASN (Autonomous System Nbr).

    Border Gateway Peering is done which used to broadcast nearest network to their connected Border Gateway.

    ASPATH - Autonomous System Path - Contains the path details and from this chose the shortest path.

---------------------------------------------------------------------------------------------------------

AWS GLOBAL ACCELARATOR: 

    It helps to select the nearest edge location (network). 

    Lower number of hops , better the experience.

    Unicast IP address - Normal IP address.
    Anycast IP address - Multiple devices can use anycast IP address.

    It uses edge location (AWS Global Network)

    CloudFront - Keep content close to customer. Works on content cache. It caches only HTTP/HTTPS product.
    Global Accelarator - Keep network close to customer. It is network product works on TCP/UDP. Doesnot cache anything. 
    
---------------------------------------------------------------------------------------------------------

SITE TO SITE VPN CONNECTION:

    It helps to connect from VPC to non AWS (onprem network/another cloud).

    It consists of Virtual Private Gateway (VGW) on AWS side and Customer Gateway(CGW) on On-prem side. VPN connection between VGW and CGW.

    Both CGW and VGW is on top of router.

    Communication is encrypted in transit by using IPSec tunnel. It runs over the public internet except direct connect. 

    VGW has 2 end points in different availability zone. 

    On AWS Side, have 2 VPN connection to customer side for high availability. 

    NOTE: Only AWS Side have high availability, to make it fully high available, requires 2 customer gateway in on-prem side.

    Static VPN   -   Static networking configuration like IP address of CGW and VGW need to maintain on both side.
    Dynamic VPN  -   If BGP (border gateway protocol) enabled, then only can use the dynamic IP. Route propagation on Route table helps to enable dynamic. 

    STEPS: 
        1. Collect the IP address of VPC and Router details in AWS and Onprem side.
        2. Attach the VGW in public zone of AWS and CGW in Router of customer side.
        3. In VGW, created with 2 endpoints in different AZ. This end point is used to connecto to CGW.
        4. Create a static or dynamic VPN connection.
        5. VPN connection is linked to VGW endpoints and this endpoint is connect to CGW.
    
    Limitation: 
        1. To check speed limit of 1.25 gbps on both tunnel.
        2. It transit through internet and latency can cause issue.
        3. Hourly cost, data transfer cost.
    
    NOTE: It can setup fastly in hours. 

---------------------------------------------------------------------------------------------------------

AWS CLIENT VPN: 

    It helps to connect AWS to Client.

    Site to Site VPN connection which connects AWS to Non AWS like On-prem, another cloud.
    Client VPN connects AWS with Client.

    Client should be Open VPN supported.

    NOTE: Without Split tunnel , connects to internet through client VPN.

    XXXXXXXX - REQUIRE TO DEEP DIVE

---------------------------------------------------------------------------------------------------------

AWS DIRECT CONNECT: 

    It is physical connection beween on-prem and AWS from 1 , 10, 100gb/s.
    Business premises <> DX location <> AWS Region
    Order Port Allocation at DX location.
    Port hourly cost and outbound transfer cost. Inbound transfer cost is free.
    Takes time to provisioning. Physical labels and no resilience.
    Low & consistency latency and high speeds.

    Can access AWS private and public services and no internet connection is required. 

    Direct connect location: 
        AWS direct connect cage 
        Customer or communication partner cage. 

    Public VIF  -   Connect from onprem to AWS private services
    Private VIF -   connect from onprem to AWS private services


    DX PHYSICAL CONNECTION ARCHITECTURE: 

        DX connection - physical port of 1, 10, 100 gb/s
        Single mode FIBER not copper connection.
        1000BASE-LX Transceiver - 1 Gbps
        10GBASE-LR Transceiver - 10 Gbps 
        100GBASE-LR4 Transceiver - 100 Gbps 
        Port speed and full deuplex manually set. 
        Support BGP and BGP MD5 authentication.


    DIRECT CONNECT SECURITY - MACSec:

        Frame encryption - layer2.
        Hop by Hop between two switches/routers.
        Confidentiality & data integrity.
        Data origin authenticity.
        Replay protection.

---------------------------------------------------------------------------------------------------------

ROUTE 53 FUNDAMENTALS:

    It is AWS managed DNS product.

    It provides 2 main functions,
        1. Register domain
        2. Host zones (hosted zone) and managed nameserver.

    It is global service and have single database.

    It creates the zone file and nameserver and place the zone file in nameserver and place the nameserver details in high level domain like .org by adding name server record.

    Inside hosted zone has recordset (records)

    
REGISTERING A DOMAIN: 

    It consists of domain name registration + name server details. Both are separate things, 
        One can register a domain name in AWS or external and nameserver also can use external or AWS one.

    In AWS, create with 4 nameservers in high availability zone.


DNS RECORD TYPES:

NS      -   NameServer      -   It Points to nameserver details. 
A       -   IPV4 Address    -   It points WWW to IPV4 address.
AAAA    -   IPV6 Address    -   It points WWW to IPV6 address. 
CNAME   -   Canonical Name  -   It points to another domain. CNAME ftp CMAN www
TTL     -   Time To Live    -   How much time to active in Resolver server. 
MX      -   Main Server     -   It points to Mail Server.
    MX 10 mail 
    MX 20 mail.domain.org 

    10, 20 is priority.

TXT     -   Text Record     -   It uses to prove domain ownership.


ROUTE 53 PUBLIC HOSTED ZONE:

    It is zone file which is database of zone file. Hosted zone is located in public and access through internet.
    In Route 53, have 4 public hosted zone.
    It can be resolved through from public internet and VPC.
    Route 53 resolver is VPC + 2 ip address.


ROUTE 53 PRIVATE HOSTED ZONE: 

    Hosted zone is placed within VPC and cannot be resolved from public internet.
    Only with in VPC, can resolve the private hosted zone.

    Split view is concept of using public and private hosted zone together. Private use for internal users and public for other users. 

    In split view, few records can be public (www 12.12.13.14) and few records can be private (accouting 13.14.15.16)

    It tagged to VPC. Multiple VPC can be tagged. Only can access from tagged VPC.


CNAME VS ROUTE 53 ALIAS:    

    CNAME doesnot support Naked/apex domain. (www.google.com <> google.com)
    Naked domain is google.com
    Normal domain is www.google.com

    ALIAS is AWS supported record type which used to point naked domain.

    Generally AWS resources like ELB points to DNS name.

    A record Alias
    CNAME record Alias.


SIMPLE ROUTING:

    It points WWW A record to IP address. 
    
    It doesnot have health check.


HEALTH CHECK: 

    It performs health check which separate from record set.

    By default checks for every 30 sec.

    It checks by establish TCP connection and respond through HTTP/HTTPS.

    It checks connectivity + words (string match) available response content.

    It is of 3 types,
        end point   -   based on IP/domain. create internally in multiple regions.
        calculated health check - use other health checks with AND/OR operation.
        Use cloud watch alarm.


MULTI VALUE ROUTING:

    It is combination of simple routing and fail over routing.
    It is single domain with multiple ip address.
    When client calls, it returns 8 records (ip address) and client choose randomly.
    When health check is failed, it will not return ip address to client.


WEIGHTED ROUTING:

    It is based on weightage.
    Each record have associated health check. 
    If health check is failed, then choose another record. 


GEO PROXIMITY ROUTING:

    It is based on the bias. If bias is larger, cover larger distance. If bias is small, cover smaller distance. 
    It is based on the location of server.


GEO LOCATION ROUTING:

    It is based on record set is tagged to country or continent.
    Useful for content delivering based on country.
    IP address is used to take the country.


FAILOVER ROUTING:

    It is based on active passive type.
    Primary is attached to health check. If primary failed, it route to secondary.


LATENCY BASED ROUTING: 

    It is based on the latency or quickest server to select for the record set. 
    It consists of region tagged to server.


ROUTE 53 INTEROPERABILITY: 

    It consists of 2 parts,
        Domain registration -   Register the website in HLD using nameserver and domain name.
        Domain hosting - Create the name server and this name server used to register. 

    Both are separate process and can use AWS in either part.


=========================================================================================================
STORAGE SERVICES: 
=========================================================================================================

FSx FOR WINDOWS SERVER: 

    This is like EFS but for windows server.It is Fully managed native windows file servers.
    Designed for to integrate with file servers.
    Can integrate with directory service or self managed AD.
    Can deploy in single or multi AZ within VPC.
    It support ondemand or scheduled backups.

    Can be accessible using VPC, VPC peering, VPN connection, Direct connection.

    It supports de-duplication, DFS(Distributed file system), KMS at rest and encryption in-transit.

    Support file level versioning (volume shadow copies)

    \\fs-xxxx.domain-name\file-name 

    VSS - User driven restores 
    Native file system accessible over SMB (Server Message Block). EFS uses NFS protocol. 
    Uses windows permission model.
    Support DFS (scale out file share structure)

    AWS managed - No server admin.
    Integrates with Directory Service or your own directory.
    NOTE: Simple AD is not supported.

---------------------------------------------------------------------------------------------------------

FSX FOR LUSTRE:

    Defined specifically high performance computing Linux clients (posix)

    used for machine learning, bigdata, financial modelling.

    Deploy in single AZ.

    100 gb/s throughput and sub millisecond latency.

    Deployment types: 
        Scratch: 
            for short term. 
        Persistent: 
            for longer term , high availability, self healing.

    can be accessible over vpn or direct connect.

    At architecture level, it consists of:
        Metadata Targets (MDT)
        Object Storage Target (OST)

    It uses ENI to connect to FSx for lustre.

---------------------------------------------------------------------------------------------------------

EFS (ELASTIC FILE SYSTEM): 

    Support POSIX.

    It is based on NFSv4 (Network file system) which can be mounted oon EC2.

    It helps to store data outside of EC2 which doesnot lost.

    Same EFS can be mounted on multiple EC2 instances.

    EBS is block storage but EFS is file storage.

    Can be accessed from onprem using VPN or direct connect, vpc peering.

    It consists of,
        General Purpose 
        Max IO 

    Throughput modes,
        Bursting 
        Provisioned 

    Storage Classes, 
        Standard 
        Infrequent access

    Can use lifecycle policies to move data between different classes.

---------------------------------------------------------------------------------------------------------

S3 OBJECT STORAGE VS EBS BLOCK VS EFS FILE STORAGE: 

    OBject storage think of any videos, pictures, etc.
    Block storage is attached to single instances.
    File storage is attached to multiple instances.

---------------------------------------------------------------------------------------------------------

S3 OBJECT STORAGE CLASS: 

    From s3 standard to s3 glacier , can move in 1 day.
    In life cycle policy. move from IA to glacier only after 30 days.
    For CRR, only support KMS & SSE-S3.
    In CRR, objects not owned by source bucket are not replicated , replication is not retroactive or replication doesnot support SSE-C

    S3 STANDARD: 
        It is default storage class and object data is replicated across 3 AZ.
        It provides 99.99999999999 (11 9s) durability.

        data store is cost + out data transfer cost.
        It should be used for frequently access data.

    S3 STANDARD IA: 
        It is same as standard.
        It includes retrieval fee.
        It has minimum duration charge of 30 days.
    
    S3 ONE ZONE IA: 
        It stores the data in single AZ.
        Use only for data can be regenerate or lose is fine.
        It has minimum duration charge of 30 days like IA.
    
    S3 GLACIER INSTANT RETRIVAL: 
        Its like standard IA.
        Minimum storage duration charge is 90 days.
        More expense retrival costs.
        Data can be accessed instantly and doesnot require any retrival process like standard s3 but higher cost.

    S3 GLACIER FLEXIBLE RETRIVAL:
        Cost is 1/6th of standard s3.
        It requires retrival process to get the object.
        Expedited - 1 to 5 mins
        Standard - 3 - 5 hrs 
        bulk - 5 to 12 hrs 
        Cant make the object public and data not available immediately.
        
    S3 GLACIER DEEP ARCHIVE: 

        180 minimum billable duration.
        Jobs retrieved to standard IA and take much longer time to retrieve.

    S3 INTELLIGENT TEARING: 

        Object moved from one class to another depends upon duration of access.


    S3 LIFE CYCLE CONFIGURATION: 

        It provides different capabilities, 
            move current version between storage class 
            move non current version between storage class 
            Expire current version of objects 
            permanently delete non current version of objects 
            delete expired object delete markers or incompatible multipart upload.

        Can move from one storage class to another, 

            Standard IA     -   mininum 30 days
            Glacier Instant Retrieval - Minimum 30 days + standard ia retrival days. 
            Glacier flexible retrieval - Minimum 90 days
            Glacier deep archive - Minimum 180 days.


    S3 REPLICATION:

        It is used to store the data in another account or another region.
        Versioning should be enabled on both source and destination.
        Can select all objects or use prefix to limit or tags.

        It consist of,
            SRR - Same Region Application
            CRR - Cross region Replication 

        For cross account replication, requires bucket policy to trust the role from source.

        NOTE: Default storage class is standard but can override to choose other class.

        RTC (Replication Time Control) - have SLA of 15 mins.

        NOTE: Can replicate encrypted objects using SSe-S3, KMS but cannot replicate objects which encrypted by using SSE-C 

        Delete will not be replicated.


    S3 OBJECT ENCRYPTION: 

        NOTE: Only objects can be encrypted but bucket itself cannot be encrypted. 

        Server Side Encryption:
            Plain text data is uploaded by user and then encrypted and store by s3. s3 handle the encryption.
        
            SSE-S3  -   AWS handles both keys and Encryption/Decryption process.
            SSE-C   -   Customer is responsible for keys. Encryption/Decryption done by S3.
            SSE-KMS -   Key is handled separately by KMS. Customer managed key is generated. Used that key for encryption/decryption. It gives role separation by restrict access to CMK and rotation control. 

            AES256 algorithm used for encryprion.

            In CLI or API, while upload objects require to specify Header x-amz-server-side-encryption, with AES256 for SSE-S3 , aws:kms for KMS.  Default is AES256. 

        Client Side Encryption: 
            Data is encrypted by user and then upload the encrypted data in s3. 


    S3 PRESIGNED URL: 

        It is giving access to another person/application to access to private s3 bucket.
        If any unknown user wants to access temporarily to private s3 bucket.

        Process: 
            1. User/application send the "generate presigned url" request along with permission and expiry time to s3 bucket.
            2. s3 bucket generate the presigned url and return to user/application.
            3. From application, can return to user. 
            4. NOTE: While generate url request, application/user connected role will be sent.

        1. Can create presigned URL eventhough no access to object with expiry date.
        2. When use the URL, permission is using the identity permission.
        3. Dont generate presigned url with role because have temporary credential of short expiry.


    S3 SELECT AND GLACIER SELECT: 

        Both are used to retrieve parts of objects, pre filtered by s3.
        It is SQL like statements.
        It support CSV,JSON, Parquet, BZIP2 compression.
        Since it is s3 feature, it reduce cost in data transfer and faster.


    S3 ACCESS POINTS: 

        It simplies managing access to s3 bucket/objects.
        With bucket policy, it is more complex to manage multiple teams and hundres of objects.
        Can create many access points with each have different policies with different network control(from vpc or internet).
        Each access point has own endpoint address which can be used to access instead of common s3 endpoint.
        aws s3control create-access-point 
        By using policy, can restrict the access to specific object.

        NOTE: In bucket policy can give high level permission and in Access Point to give more granular access. if bucket policy denied, it will stop there.

        Can access from VPC using VPC endpoints and grant access using endpoint policy.

    
    S3 OBJECT LOCK: 

        During bucket creation, can lock the object. 
        To object lock for existing buckets, require to contact AWS support.
        Versioning also enabled and also versions also locked.
        Once created, cannot disable the object.
        It support Write Once Read Many.

        NOTE: Object lock settings can only be specified in CLI, API or SDK.

        It support retention,

            1. Retention Period -   User can specify retention period in days/years.

                    Compliance Mode -   Till expiry, no user include account root user cant change anything including deletion, overwritten. Cant change retention period.

                    Governance Mode -   Set of users can modify the object and settings can be adjusted. 
                    s3:BypassGovernanceRetention is used or x-amz-bypass-governance-retention header.

            2. Legal hold       -   Dont set retention period. Only on/off. No changes or delete until legal hold removed. Prevent accidental deletion.

---------------------------------------------------------------------------------------------------------

AMAZON MACIE: 

    It is a data security and data privacy service.
    Discover, monitor and protect data stored in s3 buckets.
    Automated discovery of PII, PHI and financial data.
    It is multi account based generally use in organization.

    It consists of ,
        Managed data identifiers    -   Uses AI/MI 
        Custom data identifiers     -   Customer create using Regexp to match pattern in data.

    Mazie -> Schedule job -> Findings -> Event bridge -> Security hub / Lambda.

    There are 2 types of findings, 
        Policy findings - It means encryption disabled, publicaccess disabled, ACL changes policy.
        Sensitive data findings
    
---------------------------------------------------------------------------------------------------------

EBS VOLUME TYPES:

    Deploy in single AZ.
    Support POSIX.

    GENERAL PURPOSE SSD: 
        It consists of GP2, GP3.
        IOPS is one chunk of 16kb in one second.
        Baseline rate - 100 IO credits per second to fill credit.
        Beyond 100 IO, 3 io rate per gb per second.

        GP2: (default)
            SSD based.
            From 1 GB to 16 TB.
            Get 3 IOPS per second per gb.
            Can burst upto 3000 IOPS.
            Great for boot volumes, data backups and database loading.
    
        GP3:
            SSD based.
            3000 IOPS.
            Cheaper than GP2.
            Offers high throughput.

    In gp2, dependsup on the size, iops and thorughput changes. As user dont have option to chose these. With GP3, removed this limitation and user can change iops/throughput. 


    PROVISONED IOPS: 

        It consists of IO1, IO2 & IO2 block express. 
        Achieve a max of 64000 IOPS. upto 256000 for block express.
        Used for low latency application, sub milli second latency requirement.

    HDD (Hard Disk Drive) Based: 

        st1:
            Throughput optimized.
            Read sequential data.
            Max 500 IOPS.
            Useful for bgigdata, data processing.

        sc1:
            Cold HDD.
            For archival data.
            Max 250 IOPS.
            Useful for less frequently accessed data.

    
    INSTANCE STORE VOLUMES: 

        Support POSIX.
        It provide block storage devices.
        NOTE: Instance store volume can be attached only at launch. Unlike EBS, cannot attached after launch instances.
        If instance moves from one host to another, data will be lost.

        It is local on EC2 host.
        Add only at Launch time.
        If instance move, resize, hardware failure, data is lost.
        It gives high performance.
        Temporary data store.


    INSTANCE STORE VS EBS VOLUMES:

        EBS: 
            Data is persistent.
            Resilience
            Have snapshot option.
            Storage isolated from instance cycle.
            
        Instance Store:
            Data is not persistent.
            Not Resilient
            best Cost 

    Pure Cost efficacy - Cheap - st1 or sc1 
    Throughput -    st1.
    Boot - not hdd.
    gp2/3 - upto 16,000 iops.
    io1/2 - upto 64,000 iops.
    io2 block express - 256,000 iops depends upon larger volume size.
    RAID0 + EBS - 260,000 iops.
    more than 260,000 iops - instance store volume.

=========================================================================================================
COMPUTE, SCALING & LOAD BALANCING: 
=========================================================================================================

REGIONAL & GLOBAL AWS ARCHITECTURE:

    Global service location & discovery.
    CDN and optimization.
    Global health check and failover.

    Global: 
        Route 53 - DNS Server 
        CDN - Content delivery caching which close to customer.
    Regional: 
        Most services are regional.

        It consists of,
            web tier -  
            app tier -  ec2, lambda 
            storage tier - ebs, efs
            caching layer - DAX (dynamodb cache), elastic cache, redis, memcache
            db tier - redshift, rds etc. 
            app services - sqs, sns


EC2 PURCHASE OPTIONS: 

    On-demand: 
        It is default and pay only for use.
        per second billing only when instances is running.
        It doesnot give any priority access when AWS failure.
        It is suitable for short term workload.

    Spot Pricing: 
        It is cheapest.
        Any ununsed compute is released to AWS.
        AWS set the spot price and customers can quote the price. If customer price is less, then spot instances will be terminated.
        Should not use for critical application which doesnot tolerate interruptions.
    
    Reserved Instances: 
        Longterm usage 
        Unused cannot be released.
        Reservation for 1 year or 3 year
        Payment options,
            No upfront      -   less discount.
            partial upfront -   medium discount.
            Full upfront    -   high discount.
        Schedule Reserved Instances: 
            When require long time environment with specific timing.
            min 1 year commitment.
        Capacity Reservations:
            It is useful when cant tolerate interruptions.
            It consists of billing + capacity component.
            It is of,
                Regional reservation    -   region level across az.
                Zonal reservation   -   at single az level.
                Ondemand capacity reservation - still pay eventhough not use.
        Savings Plan:
            1 or 3 year commitment in hourly spend.
            
    Dedicated Host: 
        Purchase the hosts and instances are hosted in hosts.
        Pay for the hosts and not on instances.
        Use for software license based on sockets or cores (physical machine).
        Only your instances runs on hosts.
        underlying host is customer managed.

    Dedicated Instances:
        Instances are placed in host. No other customer instances are placed under same hosts.
        Pay for entire hosts + instances charge per hour.
        Underlying host is aws managed. 

---------------------------------------------------------------------------------------------------------

EC2 NETWORKING:

    When EC2 created, primary ENI is created automatically. Secondary ENI can be attached during instance creation or added later.
    NOTE: ENI can be in different subnet but should be same AZ.
    Security groups are associated with ENI not EC2.

    ENI: 
        primary private ip4 address.
        1 or more secondary private ip address
        secondary eni can be moved between instances.

    Public IP address is assigned based on subnet settings.

---------------------------------------------------------------------------------------------------------

BOOTSTRAPPING VS AMI BAKING:

    Bootstrap is script placed in userdata. It is flexible but takes time.

    AMI Baking is preconfigured with base os and application. It is used to deploy many instances. AMI to be created from instance. 

    Best results can be achieved by combining both.

---------------------------------------------------------------------------------------------------------

ELASTIC LOAD BALANCER:

    It accept the connection from customer and distribute connection to registered target compute.
    User abstracted from infrastructure.
    It support, 
        IPV4 only
        Dual Stack (Support both IPV4 & IPV6)

    Types,
        Internet facing     -   assign public and private ip address.
        Internal            -   assign private ip address.
    
    NOTE: Loadbalancers connect to both public and private EC2 instances irrepective of type chosen.

    It require 8 or more free IP adddress , /28 or /27 subnet are used for load balancing.

    Cross Zone Load Balancing: 
        When traffic received by elb node, it transfer the traffic to target only within their AZ and cause uneven distribution of traffic.
        With this cross zone, can send the traffic to target system in another AZ and useful for evenly distribution of traffic.


    SESSION STATE: 

        It is server side piece of information.
        Persists while interact with application.
        Session is created when interact with application and session state is stored.
        Ex: Shopping cart added.
        Session data can be stored internally in server or externally stateless server.

        External server: 
            keep separate server and instances uses this external server to get session data of the customer.
            It helps when lloadbalancer choose different instances, it wont effect customer experiences.


    APPLICATION VS NETWORK LOAD BALANCER:

        ALB:
            It is layer 7 which listens on HTTP/HTTPS.
            IT doesnot support SSH, SMTP, gaming etc.
            It has to listen using HTTP/HTTPS listener. No TCP/UDP listener.
            Can understand layer 7 content like cookies, path, user location, custom header.
            SSL connection is terminated in ALB and new connection established from ALB to application instances.
            Must have SSL certificates installed on ALB if HTTPS used.
            HTTPS slower compare to HTTP.

            Rules are used to direct connections to target.
            Rules processed in priority order.
            Rule condition is host-header, http-header, http-request-method, path-pattern, query-string and source ip.

            have security group and do connection termination.

            Actions can be forward, redirect, fixed-respone, authenticate-oidc, authenticate-cognito

        NLB: 

            It is based on layer 4 and understand TCP, TLS, UDP protocol.
            No header, No session stickiness, no cookies understand.
            It gives faster response.
            It support SSH, gaming, SMTP
            In health check, just check ICMP/TCP handshake 
            Can have static IPs and use for whitelisting.
            It support end to end encryption to instance through TLS.
            Used with private link.
            No security group and no connection termination.

    SESSION STICKINESS:

        in ALB, stickiness enable at target group level.
        ALB send the session details in cookies (AWSALB) to browser with expiry time. When next call, uses that cookies, reach to the specific instances.
        It may cause uneven load distribution to instances.
        If instance failure, load balancer doesnot send to failed instances.


    CONNECTION DRAINING: 

        Connection draining - Allow inflight request to complete and no new request are allowed.
        It is supported only on CLB only and on load balencer level.

        Deregistration delay - It is supported in ALB, NLB, GLB on target group level. Stop the load balancer to send the request to deregister target. Default is 300s upto 3600s.


    X FORWARED FOR AND PROXY PROTOCOL:

        When ELB is used, it send its own IP address instead of client IP address to servers.

        x-forwarded-for:client 
            This is http header. Header is added by proxies/LB.
            This header will contains original client ip address and any other proxies.
            Since layer 7 , only supported in ALB & CLB.
            It support only HTTP/HTTPS.

        proxy protocol:
            It works on layer 4, TCP.
            generally use with NLB.

---------------------------------------------------------------------------------------------------------

ASG REFRESHER:

    Automatic scaling and self healing of ec2 instances.
    Uses launch template or launch configuration.
    Has minimum, desired, maximum value.
    Keep running instances on desired. 
    Scaling policies are used to automate based on metrics.
    Manual scaling by adjust the desired capacity.
    Scaling types are,
        Scheduled scaling 
        Dynamic Scaling: 
            Simple - based on metric like cpu.
            Stepped scaling - multiple step metrics value.
            target tracking - maintain the target percentage of metric like cpu.

    Cool down period - how long to wait before another scaling action to take place.

    By default, uses ec2 status check. Self healing will replace failed instances automatically.

    Can integrate with target group of ASG.

    ASG can configure to use ELB health check instead of EC2 stautus check.

    ASG defines when and where to launch instances, Launch configuration define what instances to define.

    It consists of Launch Template & Launch configuration. Launchc template can have multiple version and LC is immutable. Also LT provide more EC2 options like dedicated host. LC is legacy.

    ASG LIFE CYCLE HOOKS:

        It helps to setup custom actions on instances during ASG actions like instance startup or terminate.
        Hook can integrate with SQS, SNS, Eventbridge.

        Launch -> pending -> Pending wait (3600s default) -> Pending proceed -> Inservice (Continue using CompleteLifeCycleAction)

        Terminate -> Terminate wait (3600s default) -> Terminate proceed -> Terminated

---------------------------------------------------------------------------------------------------------

EC2 PLACEMENT GROUPS:

    How ec2 is placed in physical hardware.
    It consists of 3 types,

        Cluster placement group:
            For highest performance.
            Create a group and place all together in same group. 
            Best practice is using same type of instance and launch all together.
            All place in an single AZ.
            Use the same rack.
            Offers single stream data connection with 10gb/s.
            If hardware fails, all instances down.

        Spread:
            It ensure maximum availability and resilience.
            Spread across AZ.
            Each instance has separate rack.
            Limit of 7 instances per AZ.
            Not supported for dedicated instances or host.

        Partition:
            Similar to spread.
            It is multi AZ.
            Create a multiple partition with each limit of 7 instances.
            Each partition has its own racks.
            Useful for huge scale parallel processing.
            Instances can be place in specific partition.
            Great for topology aware application like HDFS, Cassandra etc.
            Impact only at partition level.

---------------------------------------------------------------------------------------------------------

GATEWAY LOAD BALANCER:

    From public subnet which connect to internet, requires security appliances which filter inbound and outbound data to subnet. This security appliances cannot scale well dependsupon instances.

    GLB helps to run and scale 3rd party security appliances like firewall, intrusion detection and prevention system.

    It consists of,
        endpoints   -   traffic enters/leaves in this.
    
    It uses a protocol GENEVE means packets are not altered. 

    Client -> GLB endpoint -> GLB (Use GENEVE protocol) -> Security Appliances.

=========================================================================================================
MONITORING, LOGGING & COST MANAGEMENT:
=========================================================================================================

CLOUD WATCH: 

    Cloudwatch consists of metrics, Alarm, logs and events.
    It is in public service endpoint so can access from on-prem.
    From anything inside of instances, use cloudwatch agent.
    From public subnet, can route to internet gateway to cloudwatch endpoint.
    From private subnet, conneect using VPC interface endpoints.

    namespace - container for metrics.
        AWS/EC2 , custome-name/metrics 
    Datapoint - Timestamp, value, unit of measure
    Metrics - Tiem ordered set of data points.
    Dimension - it is key value pair: 
            It helps to aggregate the data.
            Also helps to differentiate between different instance using key value pair.

    Standard Resolution - 60s granularity. 
    High resoulution - 1s granularity.

    Retention dependsupon the resolution, 
        For < 60s - retain for 3 hrs
        For 60s - retain for 15 days
        For 300s - retain for 63 days
        For 1 hr - retain for 455 days.

    As data ages, its aggregated and stored for longer with lesser resolution.

    ALARMS: 
        Watches a metric over a period of time.
        ALARM or OK 
        Monitor the current value with threshold value maintained and trigger the SNS, ASG.


CLOUD WATCH LOGS: 

    It is region level.

    It consists of,
        Injection side - receive logs from other services.
        Subscription side

    Log event (Timestamp, raw message) -> Log stream -> Log group.

    Retention, Permission & Encryption at log group level.

    Metric is created from metric filter. 

    To export log from cloudwatch log to s3 by using s3 export function which is not real time and takes upto 12 hrs.

    To send near real time - Loggroup -> Subscription filter -> Kinesis data firehose (near real time).

    To send  real time - Loggroup -> Subscription filter -> Lambda -> Amazon elastic search.


CLOUD TRAIL: 

    It capture the API activities.
    Default stores 90 days event history and takes 15 mins to deliver events. 
    It consists of,
        Management events - create s3 bucket. 
        Data events - require to enable. uploading objects to s3. Lambda function invocation. 
        Cloud trail insights - Used to detect unusual activity in account. Analyze write events to detect unusual activity. 
    It can enable in one region or all region. For global services like IAM capture in us-east-1 

    It can integrate with cloud watch logs or store logs in s3 bucket. 

    Organizational Trail: 
        In AWS organization management account, enable coud trail, will keep all management data of all accounts.


AWS XRAY:

    It is distributed tracing application.
    It is like App dynamics and gives flow of data in different services is called service map.

    Tracing Header - It is unique trace ID generated by first service, which is used to track a request through your distributed application.
    Segments - Data blocks. 
    Sub segments - More granular data.
    Service graph - json document which detailing services and resources.
    Service map - Visual representation of service map which displays response, request time , any errors.

    X-ray agent installed on EC2, ECS. Lambda enable option, EBS pre installed, API gateway per stage option.
    Requires IAM permission.


COST ALLOCATION TAGS:

    Cost Allocation Tags have to enabled individually.
    Per Account for standard account or Master account for organization.
    It can be AWS generated (aws:createdby or aws:cloudformation:stack-name) or user generated.
    Added to resources after they are enabled by AWS.
    Both are visible in cost report and this can be used as filter.
    Can take upto 24 hours to be visible.


TRUSTED ADVISOR:

    It compares what you have and what should you have.
    It provides advices in,
        Cost optimization
        Performance
        Security
        Fault tolerance 
        Service Limit 

    7 core free checks for basic or developer support: 
        s3 bucket permission
        security group - specific port unrestricted
        IAM use
        MFA on root account
        EBS public snapshot
        RDS public snapshot
        50 service limit checks.

    For business plan,
        Access for AWS support API.

=========================================================================================================
DATABASES:
=========================================================================================================

RDS (Relational Database): 

    Database as a service
    Managed database a service which holds 1 or more databases.
    It supports, 
        Mysql, Mariadb, postgresql, microsoft sql server, oracle

    Amazon Aurora.

    Use CNAME to connect to RDS intances.
    It can be single or multi AZ.
    It is based on multiple instance type like db.m5 , db.r5 or db.t3
    EBS is attached to instances which is og gp, io , magnetic.
    Security group is attached to RDS instances.

    Transparent data encryption (TDE) for Oracle and SQL server. 
    IAM Authentication for Mysql & postgre sql.
    Cloudtrail cannot be used to track queries made within RDS.


    RDS MULTI AZ: 

        It create the standby instance in another AZ.
        From primary to standby, synchronous replication happen.
        Only user can use primary CNAME to connect to instance.
        If any error in failover, endpoint change to standby with lag of 60 to 120s.
        NOTE: MultiAz occur only within same VPC, Region, another AZ.
        Backups taken from standby and no impact on primary.
    

    RDS BACKUP AND RESTORES:

        RPO - How much data can be lose.
        RTO - With how long can withstand downtime.

        Backup consists,
            1. Automated backup - backup occur during backup window. 
                every 5 mins transaction logs written to s3.  gives rpo of 5 mins.
                retention is 0 to 35 days.

            2. Manual snapshot - delete rds instances will not delete manual snapshot.

        Restores, 
            Creates a new rds instances.
    

    RDS READ REPLICAS:

        It provides performance benefits.
        Read replicas are used only for read operations.
        From primary, asynchronous replication to replica instances.It could cause delay in replica.
        Cross region read replica - in another region.
        can have 5 read replicas per db instance.
        Read replicas can have another read replicas, but have lag problem.
        Can promote read replica to primary instance.
    
    
    RDS DATA SECURITY:

        Authentication - map rds user with iam identity.
        Authorization - it handle internally in rds.
        Encryption at rest - Support KMS & EBS encryption. 
        Encryption in transit - encrypted via ssl/tls. 


    AURORA ARCHITECTURE: 

        It uses a "CLUSTER"
        It consists of single primary instances + 0 or more replicas.
        Replicas inside aurora provide both availability + read performance.
        It uses a shared cluster volume. Maintains 6 storage node across AZ. High IOPS, low latency.
        Can have upto 15 replcas.

        It have multiple endpoint. Cluster endpoint , Reader endpoint. 
        It is compatible with mysql & postgresql.
        Backtrack is to allow to point in time rollback.
        It gives faster clones.


    AURORA SERVERLESS: 
        
        No need to static provision instance or size.
        ACU - Aurora Capacity Units, with min and max acu.

    AURORA MULTI MASTER:

        In default aurora, have single master, single write and multiple read.
        In multi master, have both write and read. 

---------------------------------------------------------------------------------------------------------

DYNAMODB: 

    It is nosql wide column. It is based on key value pair and document.
    It is multi AZ and global.
    Single digit milli seconds.
    Backups, point in time recovery.

    NOTE: dont increase the RCu & wCU and then decrease. Due to partition change cause long time performance issues. 

    Items - Row 
    Keys are of simple primary key or composite primary key.
    Item is max size of 400 kb.
    It is of provisioned and ondemand capacity.
    WCU (write capacity unit) - 1kb/s
    RCU (Read capacity unit) - 4kb/s

    Ondemand backup - similar to manual rds backup to same or cross region.
    point in time recovery - disabled by default and to enable at table level. gives 35 day recovery window with 1 second granularity.

    Query: 
        query single item based on key
    Scan:
        least efficient and expensive
        filter non partition key attributes.
    
    Consistent Model: 

        Eventual Consistent Read:
            It is not guarantee to get latest data.        
            half the rcu cost.

        Strong Consistent Read:
            Gurantee to get latest data from leader node.
            4 kb/s 


    DYNAMODB INDEXES:

        LSI (Local Secondary Index):            
            It allow multiple query.
            same partiton key + different sort key. 
            It must be created during base table created, cannot add later.
            5 LSI per table.
            Shares the RCU & WCU of base table.
            Attributes - All, key only, include.

        GSI (Global Secondary Index):
            Allow differnt sort key and partition key. 
            Can be created at any time.
            20 per base table.
            Have own rcu & wcu
            It is always eventual consistent because load data from base table.
        
    
    DYNAMODB STREAMS & TRIGGERS:

        It is time order list of item changes in table.
        Enable on per table basis.
        Records update, delete , insert.
        Different view types influence what is in the stream.
                Key Only, New Image, Old Image, New and Old Image.
        
        Triggers are used to trigger based on the event in dynamodb.
        Can trigger lambda in response to data change.
        It is useful for reporting or analytics, data aggregation, messaging ot notification.

        streams + lambda -> trigger 


    DYNAMODB ACCELERATOR (DAX): 

        inmemory cache for dynamodb
        retrive data in microseconds.
        It is placed multi AZ and consists of primary and replica nodes.
        Primary node (write) and replica node (read)
        Dynamodb is in public but DAX is within VPN.
        Supports write through.

        Not suitbale if require strong consistency read.

        Tradidional cache:
            application -> Cache (cache hit)
                        -> database (cache miss)  -> application -> cache (to store in cache)

        Dax: 
            application (dax SDK) -> dax -> dynamodb  (dax handle the request to db and store response before send back to application)


    DYNAMODB GLOBAL TABLES: 

        It provides multi master cross region replication.
        Read and write occurs to any region.
        Strong consistent only in region which writes.


    DYNAMODB TIME TO LIVE(TTL):

        Attributes have ttl to live and deleted after.
        Pick and attribute to use for ttl process which contain timestamp in epoch.
        First set to expired and then to deleted.
        Can also configure a stream on TTL.

---------------------------------------------------------------------------------------------------------

AWS ELASTICSEARCH:

    It is part of ELK stack.
    Managed implementation.
    It runs with in VPC.
    It provides search and indexing services. 

    kibana - visualizationn/dashboard 
    logstash - similar to cloudwatch log. logstah agent is required to installed.

---------------------------------------------------------------------------------------------------------

ATHENA: 

    It is serverless query service to access data in s3 using SQL statement.
    Pay for data consumed.

    It has no infrastructure.
    doesnot require ETL process.
    Querying vpc flow logs, logs in S3, AWS glue data log & webserver logs.
    Athena Federated query can source non s3 using connector.

---------------------------------------------------------------------------------------------------------

AMAZON NEPTUNE:

    It is managed graph database.
    It is based on relationship between data.
    It is very much like RDS.

    graph style data.
    fliud relationship, social media.
    fraud detection.
    recommendation engine
    network and it operations.

---------------------------------------------------------------------------------------------------------

AMAZON QUANTUM LEDGER DATABASE:

    Part of blockchain range of products.
    It is ummutable append onlt ledger based db.
    It is serverless.
    Also stream data to kinesis.
    Document DB model based on json model.
    ACID - Work all or none.


---------------------------------------------------------------------------------------------------------
DATA ANALYTICS:
---------------------------------------------------------------------------------------------------------

KINESIS DATA STREAMS:

    It is a scalable streaming service.
    Producers send data in kinesis stream.
    Public service and highly available.
    Stream stores 24 hrs of streaming data.
    Support multiple consumer read data with different rates.

    Kinesis is of shard architecture. Starts from 1 shard. 
    Shard support 1m/b ingestion, 2m/b consumption.


KINESIS DATA FIREHOSE:

    Kinesis stream doesnot store data.
    Kinesis firehose used to store data for long term.
    It is fully serverless and scale.
    Near real time delivery (60 seconds)
    Support transformation of data on the fly using lambda.
    Deliver to HTTP endpoints, S3, Redshift, Elasticsearch.
    Producer can be stream or other source.


KINESIS DATA ANALYTICS:

    It is real time data processing product using SQL.
    Source can be streams or firehose or s3 bucket.
    Destination can be firehose, Lambda, Kinesis data streams, 

---------------------------------------------------------------------------------------------------------

MAPREDUCE:

    It is based on split, split into smaller parts for computation.
    After computation, merger to get the output.
    Uses HDFS (Hadoop)

---------------------------------------------------------------------------------------------------------

EMR (Elastic Map Reduce) ARCHITECTURE: 

    Can be long term or short term (transient) clusters.
    Runs in one AZ only with in VPC.
    Can use spot, reserved , on demand, instance fleet.
    Used for big data processing.
    Source can be s3 
    Destination to s3.
    For intermediate storage, uses HDFS for high IO or EMRFS s3.

    Cluster consists of master nodes upto 3 master nodes. 
    1 or many Core nodes which helps to track and run tasks which runs HDFS.
    Task node - only runs tasks. 

    EMRFS - File system backed by s3 and provides resilient against failure of specific AZ. 


---------------------------------------------------------------------------------------------------------

REDSHIFT ARCHITECTURE:

    It is column based, petabyte scale, DWH product.
    It is used for reporting and analytics.
    RDS is OLTP but redshift is OLAP.

    Redshift spectrum - Direct query s3. 
    federated query - query directly in remote source like other db.
    Integrated with AWS tooling like Quicksight.
    SQL like interface like jdbc/odbc connections.

    It is NOT serverless.
    Runs in one AZ.
    Leader node , Compute node.

    Enhanced VPC Routing - Traffic is routed based on VPC networking like using security group, NACL etc.

    Automatic backup to s3 for every 8 hrs.
    Manual snapshot to L3 
    Data is copied to additional node.

    DMS to migrate data to redshit. From kinesis firehose, to redshift.

    Backups: 
        Automatic backup for every 8 hrs or 5gb and configurable upto 35 days.
        Manual snapshot.
        Snapshots to another region.

---------------------------------------------------------------------------------------------------------

AWS BATCH: 

    It is managed batch processing.
    Job is a work which runs in container in AWS.
    job definition - metadata for the job.
    job queue - jobs are submitted to queue and wait for compute environment.
    compute environment - configure instance details.

    Source can be lambda, step functions, event bridge, 
    Destination can be s3, dynamodb, Step functions, Event bridge, 

    lambda has execution limit of 15 mins, AWS batch has no limit.
    lambda has limited disk space. 
    Batch is not serverless , uses containers. No time limit. 

    Compute resource consists of Fargate(AWS Managed) and Customer managed.

---------------------------------------------------------------------------------------------------------

AWS QUICKSIGHT:

    It is business analytics/business intelligence service.
    visualization or dashboard. 
    Source can be AWS (Athena, Redshift, redshift spectrum, sql, teradata, twitter, github) or external data source. 


=========================================================================================================
APP SERVICES, CONTAINERS & SERVERLESS:
=========================================================================================================

INTRODUCTION TO CONTAINERS: 

    dockerfile
    container 
    host 


ECS CONCEPTS:

    Elastic Container Service
    It consists of partial (ec2 mode) or fully managed (fargate)

    ECS Cluster - From where container runs.
    Container definition - where image is located, port.
    task definition -  cpu & memory, task role. By default not high available. 
    ECS Service - Service definition - defines the service - how to task scale, deploy load balancer to different tasks. selecr no of tasks. 


ECS CLUSTER TYPES: 

    EC2 Linux & EC2 windows: 
        Gives manually instance type, security group, ondemand or spot, no of instances, vpc.

    Fargate (Networking only): 
        Have option to create a new VPC.
        No need to manage ec2 instances.
        Tasks and services running in shared fargate infrastructure and these injected into VPC using ENI.
        Only pay for the container resources.


---------------------------------------------------------------------------------------------------------

SIMPLE NOTIFICATION SERVICE (SNS): 

    It is in public zone. 
    It is PUB/SUB system in AWS.
    It is POP, SOAP messaging service.
    Messages are 256<= size.
    Coordinates the sending and delivery of message.
    SNS Topics used to configure and permission.
    SNS send message of <=256 size to TOPICS.
    In Topics, user subscribed it who will receive message.
    In TOPIC, target can be HTTPS & HTTPS endpoint, email address, SQS, Mobile push notification, SMS message & Lambda.
    It is integrated with multiple AWS like cloudwatch, cloudformation, auto scaling group.
    Also possible to apply filter on a subsciber. 

    Fanout Architecture: 
        Single SNS -> Multiple SQS Subscriber. 

    Delivery status - Send the status of delivery to subcribers. 
    Delivery Retries 
    HA & Scalable.
    SSE encryption

    support cross account by using topic policy.

---------------------------------------------------------------------------------------------------------

SIMPLE QUEUE SERVICE (SQS):

    It is public service , fully managed and highly available.
    It consists of standard and FIFO.
    Upto 256kb 
    Client -> Queue -> Client poll.

    Visibility timeout - Client poll the message and process it and then to delete the message. Once client polled, it will be hidden for some time is called visibility timeout and client must process and delete from the queue. If failure, same message will visible for other queue and can process again.
    Visibility timeout - default 30s, support from 0s - 12 hrs.

    Dead Letter queue - Problematic message is send to separate queue.
    Used to decouple the application.
    ASG can scale based on queue length. Lambda can be invoked when message appear on queue.
    
    Billed based on request.
    Short(immediate) and Long (waittimeseconds) polling.
    Encryped at rest and intransit. 
    Access to queue is based on identity policy or queue policy (similar to bucker policy used for cross account)

    Standard: 
        Not guarantee to receive in order. 
        atleast once delivery 
        
    FIFO: 
        First In First Out 
        It guarantees the order. 
        exact once delivery 
        performance is limited, 3000 messages/s
        It has valid fifo suffix.

    
    SQS EXTENDED CLIENT LIBRARY: 

        SQS message size limit of 256kb. This helps to process payload > 256kb.
        Larger payloads stored in S3 and store the s3 link in payload.

        Send message - upload to s3 and send link.
        Receive message - using link to download from s3.
        delete message - delete data in s3.

    
    SQS DELAY QUEUE: 

        Allow you to postpone the delivery of message to consumers.
        Once message placed in queue, it will not available for the given period to consumers.
        Default is zero and mximum of 15 mins. 
        Not supporte in FIFO.


    SQS DEAD LETTER QUEUE:

        It helps to handle failure (if message not processing)
        Based on receive account attribute, if failed (receivecount > maxreceivecount) then move to dead letter queue.
        NOTE: Message remain only on retention time which is based on NQ time (updated on first message placed)

---------------------------------------------------------------------------------------------------------

AMAZON MQ:

    It is like merge between SQS & SNS but using open standard.
    SNS/SQS uses AWS API. SNS uses TOPIC , SQS uses QUEUES. both public services and highly scalable.

    Companies already have queing and topics system. AWS MQ based on open standard which support on-prem queue.

    It is NOT PUBLIC and runs within VPC. Doesnot have native integration with other AWS services.

    Since this is not public, to integrate on-prem to MQ requires direct connect, vpn connection.

    If API such as JMS, Protocol such as AMQP, MQTT, Openwire and STOMP are needed.

---------------------------------------------------------------------------------------------------------

LAMBDA: 

    Lambda is function as a service which runs short lived and focussed  and run up to 15 mins. 
    It uses a run time (python)
    Functions are loaded and run in runtime environment.
    Directly allocate memory which indirectly allocates CPU.
    Billed for the duration its running.
    It is key part of serverless architecture.
    Lambda storage can be extended by using EFS. 

    NOTE: Enable function URL - used to create lambda with HTTP/S endpoint. 

    It consists of,
        language to run
        deployment package 
        run time environment

    It support run time of, 
        python, ruby, java, go, c++, nodejs 

    NOTE: Can use container IMAGES with lambda. 
    Custom runtime allow additional run time like Rust.
    When lambda is invoked everytime create new environment as stateless.
    memory from 128mb to 10240 mb.
    has disk memory of 512 mb in /tmp.

    Uses IAM roles to assume to interact with AWS services.

    It used to integrate with S3, API, Lambda, dynamodb stream, dynamodb trigger, eventbridge, kinesis.


    LAMBDA NETWORKING:

            Public: 
                default connection.
                can access aws pulic services and public internet.
                have no access to access within VPC.

            Private(VPC): 
                Lambda is configured to run within VPC.
                Cant access outside of VPC unless network configured.
                It can use endpoint to connect aws public services and use nat/internet gateway to access internet.

                NOTE: It doesnot actually run within VPC but similar to fargate. From lambda service VPC, it creates ENI in customer VPC. For per subnet per security group uses the single ENI. First invocation it takes 90s delay but after no delay.

    LAMBDA PERMISSION: 

        execution role - used by lambda to assume role to access aws resources.

        NOTE: It has Resource policy which is similar to bucket policy. It cant be changed using UI but only through API & CLI. To allow external resources to access Lambda.

    LAMBDA MONITORINIG: 

        uses cloudwatch, cloudwatch logs, xray 

        Logs from lambda executions stored in logs. This require lambda execution role to give cloudwatch permission.

        Any metrics like no of invocation is captured in cloudwatch.

        Distributed tracing is captured in xray.


    LAMBDA INVOCATION: 

        Synchronous: 

                If CLI/API/API gateway  (request) -> Lambda   ->  CLI/API/APigateway (response) 
                CLI/API wait for the response from lambda.
                API gateway receive response from lambda for the request.
                Any error or retry to handle in client side.
    
        Asynchronous:

                It is typically used by AWS.
                s3 event  (request) -> lambda (failure and retry handled in lambda)
                It doesnot wait for response, fire and forget. 
                If any failure, it send message to dead letter queue.
                It support destination to SNS, SQS, Lambda, Eventbridge for both successful and failure. 

        Event Source Mapping:

                Typically used on stream or queue which dont generate event to invoke lambda.
                Kinesis , dynamodb streams, sqs 
                
                stream -> Event source mapping ( polls streams or queue for new data and get in source batch and send as event batch) -> Lambda
                Event source mapping uses execution role of lambda so requires kinesis or queue policy.

                NOTE: Invoking source permission not needed for syn or asynch invocation. In event source mapping uses execution role to poll from source, so source permission is required.

                If any failure send to SQS or SNS.

        
        LAMBDA VERSIONS: 

            Lambda can have different versions.
            A version is code + configuration.
            Version is immutable and have ARN. 
            $Latest refer to the latest version and NOT IMMUTABLE.
            Aliases point to version and this can be changed. 


        EXECUTION CONTEXT: 

            It is an run time environment on which code will be executed. 
            cold start - download dependancy + code to execute 
            warm start - use the previous execution context and run the code directly.

            After first invocation, second invocation to happen immediate on same execution context.

    
    LAMBDA HANDLER: 

        Lambda has default execution limit of 3s.

        provisioned concurrency is used to prewarm the function.

        Lambda execution environment. It has different status, 
            INIT -  Create Environment
                    Extension Init
                    Runtime Init 
                    Function Init

            INVOKE - Cold start 
            NEXT INVOKE - Warm start 
            SHUTDOWN - Terminate the environment 
                Runtime 
                Extension 

        Any code before lambda_handler goes through cold start. 
        Under lambda_handler runs for every invocation. 
        def lambda_handler(event, context);

        Handler -> lambda_function(file-name).lambda_handler (function-name)


    LAMBDA VERSION:

        $LATEST - it is latest version and can be editable.
        Functions can be published using version and this is IMMUTABLE.
        Version include code + dependancies + env variables + runtime settings.
        Version have qualified ARN and $LATEST is unqualified ARN.
                arn:aws:lambda:us-east-1:937563834:function:animal      -> unqualified (no version)
                arn:aws:lambda:us-east-1:937563834:function:animal:1    -> qualified (have version)

    LAMBDA ALIAS:

        It is a pointer to function version.
        Create alias called dev, qa, prod and attach to specific version.
        Alias can be modified to different versions.
        It is useful for blue/green or a/b deployment.

        Also allow canary routing to versions based on percentage.

    
    LAMBDA ENVIRONMENT VARIABLES:

        It is key value pair.
        $LATEST     -   latest version.
        Accessed by code during run time.
        Can be encrypted with KMS.
        Allow code execution adjusted based on this variable.


    LAMBDA LAYERS:

        Without layers, include dependency libraries like numpy as package in zip.
        Layers are stored in /opt folder.
        It consist of,
            AWS Layer
            Custom Layer 
            Specify an ARN.

    
    LAMBDA CONTAINER IMAGES:

        It is integrated with ECR to select the ECR image.
        ENTRYPOINT, CMD, WORKDIR can be overridden.

        Other configuration are same as normal lambda.

        AWS Lambda Runtime Interface Emulator (RIE) - for local test

        Generally used for CICD process, 
                Git -> Generate image with RIE -> Generate Image without RIE -> Store in ECR -> Use that image in Lambda function.
    

    LAMBDA & ALB INTEGRATION:

        Client -> ALB (convert from http to json format) -> Lambda  -> ALB (convert from json to http format) -> Client
        ALB invokes synchronously to Lambda.

        Supports Multi value header: 

            Request:  http:example.com?&search=apple&search=banana 
            without multivalue header - Lambda receives latest value alone search=banana in querystringparameter. 
            with multivalue header - Lambda receives multiple values in multivaluequerystringparameter as list.

---------------------------------------------------------------------------------------------------------

CLOUDWATCH EVENTS & EVENT BRIDGE:

    When any AWS services generate events , it is detected and trigger other actions.

    Both have default event bus in single account , bus is stream of events.

    In cloudwatch event, only one event bus available (implicit).  In event bridge, can create additional bus. 

    In event bridge, consists of rules. select source or event pattern or cron schedule and target system.

---------------------------------------------------------------------------------------------------------

API GATEWAY:

    It is a service which let create and manage APIs.
    It act as endpoint or entrypoint into application.
    It handles authorization, throttling, caching, CORS, transformations, Open API spec.
    It is public service. 
    It can withstand failure of 1+ AZ failure by default.

    It consists of following types,
        HTTP API
        Websocket API
        REST API Public
        REST API Private

    It does,
        Request     -   Authorize, Vaidate and Transform
        Response    -   Transform, Prepare and Return
        Integration -   Integrate with cloudwatch for logs, cache.

    Authentication: 
        1. Cogniti user pool: Can use cognito user pool for authentication and API gateway able to validate userpool token.
                cognito user pool -> API Gateway 

        2. Lambda AUthorization: Also known as custom authorization. From client, pass some level of authorization token. Request passed to Lambda which validate the token.
                Client -> API gateway -> Lambda authorizer (returns IAM policy and principal)

    Endpoint Types: 
        Edge optimised - Route to nearest cloudfront POP.
        Region optimised - Clients in same region.
        Private - accessible only within VPC via interface endpoint.

    Stages: 
        Deploy API as stages. 
        Each stage have unique URL to access.
        It can enable canary deployment on stages.
        Caching is configured per STAGE.

    Errors:
        4xx series - Error in client side
        5xx series - Error in server side
        400 - Bad request - Generic error 
        403 - access denied error 
        429 - API Throttle.
        502 - Bad gateway exception 
        503 - service unavailable
        504 - integration failure/timeout


API GATEWAY METHODS & RESOURCES:

    /   -   root resource
    /example    -   another resource

    Methods:
        get, put, post 

    Once resource and method is created, require to deploy to stages.


API GATEWAY INTEGRATIONS:

    Method Request
    Integration Request 

    Method Response 
    Integration Response 

    NOTE: In integrataion request/response, data can be passed directly or do transformation.
    AWS_PROXY - data pass directly without change.

    Integration Types, 
        MOCK -  used for testing. no backend involvement.
        HTTP - Backend http endpoint. Might require integration request/response change.
        HTTP Proxy - No change in integration request/response.
        AWS - Connect with other AWS services. AWS_PROXY used to send/receive data as it is. Incase of lambda, data transformation to be handle by Lambda.

    Mapping template is used for data transformation.
    Mapping template is used to,
        Modify header 
        Modify body
        Modify parameters
        Filtering
        Uses velocity template language.

    REST API  (Use mapping template) -> SOAP API 


API GATEWAY STAGES & DEPLOYMENT:

    Deployment happen as stages.
    Stage variable is like environment variable which can be point to aliases in lambda. 
    Instead of directly point to Lambda version/alias use stage variable to refer.


OPEN API & SWAGGER:

    Open API is standard. 
    Open API v2 is also called Swagger
    Open API v3 is latest version.

    API gateway support IMPORT of open api standard apis.
    API can be configured in yaml or json format.

    Export option is used to export in openapi2 or 3 format. 
    NOTE: While restore from import, it require to add resource policy in Lambda to allow invoke from API gateway.

---------------------------------------------------------------------------------------------------------

AWS STEP FUNCTIONS:

    With lambda uns only for 15 mins.
    Can build chain of function but have admin overhead and not designed for this.
    Lambda is stateless and data cannot be moved from one function to another.

    Step function is state machine which creates workflow. 
    Serverless workflow.
    Maximum duration is 1 year.\0=p\p

    Types, 
        Standard workflow - default and 1 year. 
        Express workflow  - streaming workflow upto 5 mins. 

    It can be started by API gateway, event bridge, lambda, manual etc. 
    Amazon states language based on json.
    IAL role is used for permission.

    State types,
        Succeed and Fail
        Wait 
        Choice  -   Email only, SMS only , Both Email & SMS. 
        Parallel 
        map 
        task - represent a single unit of work. 

---------------------------------------------------------------------------------------------------------

SIMPLE WORKFLOW SERVICE:

    It is legacy and step function is recommended.
    It is predecessor of step functions.
    It require external signal (decider) to intervene in process and multiple decider is required.
    1 year maximum run time.

    Step function - serverless / lower admin
    AWS Flow framework - Choose SWF.
    External signals to intervene in process - SWF 
    Launch child flows and return to parent - SWF 
    Bespoke / complex decision - SWF 
    Integrate with mechanical turk - SWF 

---------------------------------------------------------------------------------------------------------

AMAZON MECHANICAL TURK:

    It is a crowdsourcing marketplace that easier for individuals and businesses to outsource their process and jobs to distributed workforce who can perform tasks virtually.
    Tasks can be from data validation to research.
    Requester place tasks in market place -> Workers work based on interest and return get payment.
    It can be integrated with SWF.

---------------------------------------------------------------------------------------------------------

ELASTIC TRANSCODER & AWS ELEMENTAL MEDIA CONVERT:

    Elemental media convert is successor of elastic transcoder.
    File based video transcoding services.
    serverless
    Add jobs to pipelines (transcoder) and queue (media convert)
    Media convert support event bridge.

    NOTE: Media convert is used when serverless, media convertion. Used in combination with lambda and other serverless product. 
    Elestic Transcoder used when webM(vp8/vp9) format, animated gif, mp3, flac, vorbis, mav.

---------------------------------------------------------------------------------------------------------

AWS IOT (Internet of Things):

    It provides device shadow functionality.
    Rules and event driven integration with aws service.

    IOT Gateway - Endpoint through with sensors communicate.
    IOT Shadows - Store the data sent by sensors.
    IOT Rule - Can set the condition and based on trigger event bridge.
    Can integrate with kinesis to store stream data.

---------------------------------------------------------------------------------------------------------

AWS GREENGRASS:

    For example: send data from wind farm to AWS IoT cause communication problem.
    So move lot of processing locally using greengrass.
    Greengrass can communicate with IOT.

---------------------------------------------------------------------------------------------------------

SAM (SERVERLESS APPLICATION MODEL):

    It is open source framework used to build serverless application.

    Serverless Application consists of one or more serverless application like s3, cloudfront, apigateway, dynamodb etc to develop application.

    SAM Template Specification - Extension of Cloud Formation used to define the application. 
    AWS SAM CLI - Used to process template. 

    sam init        -   to initialize the project 
    sam package     -   Build zip file and upload to s3
    sam deploy      -   sam-deploy.yaml file created used to deploy application in aws. 


=========================================================================================================
CACHING, DELIVERY AND EDGE: 
=========================================================================================================

CLOUDFRONT ARCHITECTURE: 

    It is Content Delivery Network (CDN) of AWS.

    Origin - Source or original location of content.
    S3 origin or custom origin
    Distribution - unit of configuration within cloudfront.
    can have 1 or more origin.
    edge location - local cache of your data.
    Regional edge cache - Larger version of edge location and provides another layer of cache.

    Origin -> Create Distribution -> Regional cache -> Edge location

    NOTE: FOr s3 origin, regional edge cache is not used.

    It integrate with,
        ACM certificate Manager - for HTTPS 
        Download style operation only - Upload directly to origin but download from cloudfront. No write caching.

        NOTE: No write caching.

    Origin <-> Behaviour  <-> Edge location <-> Users
    Distribution contains behaviours which interact with origin based on path pattern.

    
    CLOUD FRONT BEHAVIOURS:

    Configuration:

        Price class - Which edge location to use.
        AWS WAF Web ACL -   integrate with WAF
        Alternate domain name 
        SSL Certificate 
        SNI
        Security policy 
        Supported HTTP version

    Behaviours: 

        Distribution can have multiple behaviour. Each behaviour created with precedence. Default behaviour have low precedence of 0. 

        Path pattern
        Origin or Origin group
        Viewer protocol policy  - HTTPS only, both HTTP & HTTPS, redirect HTTP to HTTPS.
        Allowed HTTP Methods - GET, PUT, POST
        Field level encryption config 
        Cached HTTP methods 
        Min, Max, Default TTL
        Restrict Viewer Access - Restrict viewer to access. Only cloudfront signed url or cookies to access content using Trusted Signers.

        NOTE: Caching control & Restrict viewer access maintained in behaviour level.

    
    TTL & INVAIDATION:

        1. From edge location, compare the version with origin.
        2. If same returns 304 not modified. Edge location uses the cached one.
        3. If modified returnd 200 with updated object and cache this updated object.

        Default TTL - 24 hrs.
        Min & Max TTL - It is lower and upper value that objeect can have. If dont specify takes default value. This value cannot be override through header.

        origin header - cache-control-max-age (sec)
        origin header - cache-control-s-maxage (sec)
        origin header - expires (date and time)

        Cache Invalidation: 
            Invalidate the edge location and takes times which expire the cached object.
            This cost to invalidate and takes time. 

        Versioned File Names: 
            It doesnot cost.
            file_name_v1, file_name_v2 
            Use different name and use application to take specific version.
            It is differnt from s3 object versioning. 
    

    CLOUDFRONT SSL & SNI:

        Cloudfront default domain name (https://1984736363.cloudfront.net)
        SSL supported by default.
        Alternative domain name - can specify different name. 
        NOTE: Whether to use HTTPS or not, need SSL certificate to verify alternate domain name ownership.
        Generate or import certificate in AWS ACM US-EAST-1 (for global services)

        2 SSL certificates require between viewer <-> cloudfront edge, cloudfrontedge <-> origin

        SNI: 
            Server Name Indication
            SNI is TLS extension which allows to include domain name.
            Can use many SSL certificates/per host / domain name.

            NOTE: only modern browsers support SNI. Old before 2003 browsers doesnot support SNI and require to buy IP address cost 600$. 
            Only Certificate Authority signed certificate can be used but not self signed certificate.

            Viewer Protocol policy - client to cloudfront edge location.
            Origin Protocol policy - cloudfront edge location to origin 

            NOTE:   S3 origin handle certificates natively.
                    ALB - can use ACM or own generated.
                    Custom Origin - Not support ACM and only custom generated.

            NOTE: Certificate need to match the DNS name of origin (cloudfront or origin)

    
    ORIGIN TYPES & ORIGIN ARCHITECTURE: 

        Origin group is group of origin.
        Origin access identity - it gives cloudfront virtual identity to access s3.
        Origin can be s3, Media store, Media package, custom origin (web servers).

        origin protocol policy - HTTP only, HTTPS only, Match Viewer.

        origin custom header 


    CACHING PERFORMANCE AND OPTIMIZATION:

        Cache Hit 
        Cache Miss 
        Maximize the cache hit ratio.

        From request it pass,
            Object 
            Query string parameter 
            Cookies
            Request Headers

        Can choose to send all or selected to CF.
        Cache all or selected (cache whitelist) in CF

    
    OAI & CUSTOM ORIGIN:

        With s3 as static website, then CF consider as custom origin.
        NOTE: Origin Access Identity is feature only for S3 origin (Not static website s3)

        OAI is associated with CF which access S3 origin. S3 bucket policy explicit allow OAI.

        For non s3 origin: 
            1. Can use custom header to pass value along with origin protocol policy and from origin side verify this value.
            2. Create security group in custom origin using edge location IP address.


    CLOUDFRONT SECURITY - PRIVATE DISTRIBUTION:

        CF consists of, 
            public  -   content is visible to all
            private -   only private user can access means request is passed with singned cookies/url.

        In general, create multiple behaviour for public and private.

        For private, 
            A cloudfront key is created by account root user.
            Add this account to CF behaviour as TRUSTED signer. Once trusted signer added it become private

        Signed URL:
            It gives access to one object
            Legacy RTMP distribution cant use cookies.
            If client doesnot support cookies.

        Signed Cookies:
            It provide access to group of objects / group of files of particular type.


    CLOUDFRONT GEO RESTRICTION:

        Restricting the content to particular location.

        2 types of restriction,
            CF geo restriction: 
                Whitelist or blacklist specific country. 
                builtin 
                99.8% accurate.
                Applies to entire distribution.
                edge location checks the CF if any geolocation enabled. Request country code of customer in geodb and send to CF. CF returns allow or deny to edge. 
                Uses only country code for filter.

            3rd party geolocation 
                Completely customizable based on browser, country and other attributes.
                require another compute server where 3rd part involved. 
                here this server checks instead of edge location.
                NOTE: CF to be Restrict viewer access and private.
                uses other parameters for filter.


    CLOUDFRONT SECURITY - FIELD LEVEL ENCRYPTION:

        Edge location is used to encrypt specific field by using public key.
        Server requires to decrypt the message using private key.
        Data is stored in encrypted form in database.

        NOTE: In normal process, data is stored in plain text. Using field level encryption, data is stored in encrypted form.

    
    CLOUDFRONT - LAMBDA EDGE:

        It allows to run leight weight lambda at edge location.
        Only Nodejs and python are supported.
        Run in AWS public space.
        Layers are not supported.

        viewer request, viewer response, origin request , origin response 

        On viewer side, limit of 128 mb 5sec , On origin side limit of 30s 10k kb

---------------------------------------------------------------------------------------------------------

ELASTIC CACHE: 

    It is in-memory database used for read heavy high performance workload.
    It is for temporary data store,
    It consists of,
        Redis: 
            sub millisecond of data.
            support string, list etc. 
            Support Multi AZ. 
            Support backup and restore.
            Support transaction. 

        Memcached:
            sub millisecond of data.
            support only string. 
            Support single AZ.
            Not support backup and restore.
            Support multithreading. 

    Reduces database workloads.
    It also used to store session data externlly. 
    
    NOTE: Requires application code change which application require understand caching.

    Flow is,
        1. Application first check elastic cashe, if cache miss.
        2. Then fetch from DB and application write to Elastic cache.


=========================================================================================================
INFRASTRUCTURE AS CODE:
=========================================================================================================

CLOUDFORMATION PHYSICAL VS LOGICAL RESOURCES: 

    Cloudformation template - yaml or json format.
    Contains logical resources - what is required. 
    Templates are used to create stacks.
    Template can be used many times, many region or account.
    Stack creates the physical resource from logical resources.
    If stack changed/deleted, physical resource also modified/deleted.
    Stack uses template, parameters and options to create a stack.


    TEMPLATE PARAMETRS & PSUEDO PARAMETERS:

        Tamplate parameters - accept input from Console/CLI/API when stack is created or updated.
        It can be default, min & max value, allowed length, allowed patterns, no echo & type.

        Pseudo Parameters: It is AWS defined parameters like AWS::Region, AWS:AccountId etc.

    
    INTRINSIC FUNCTIONS:

        It allows to access data at run time.

        Fn::GetAtt      -   get attributes of specific resource.
        Fn::Join        -   join list to string using delimeter
        Fn::Split       -   split from string to list using delinter.
        Fn::GetAZ       -   get AZ for the given region. 
        Fn::Select      -   allows to select one element from the list.
        Fn::If,Equals,Not, Or   -   conditional 
        Fn::base64      -   Accept normal text and encode as base64 and output send to userdata.
        Fn::Sub         -   substitute the variable. 
        Fn::Cidr        -   build cidr block for networking.

        Fn::Importvalue 
        Fn::Findinmap 
        Fn::Transform 

        
        Fn::Ref or !Ref vs Fn::GetAtt:

            Ref is used to refer or substitute the value.

            For any resources, its return main primary value. !Ref is used to refer the main value of physical resources returned.
            For any resources, its secondary value is referred by using !GetAtt

        Fn::GetAZ vs Fn::Select :

            Fn::GetAZ "us-east-1"  returns ["us-east-1a","us-east-1b","us-east-1c"]
            Fn::Select[0, list]

        
    CLOUDFORMAION - MAPPINGS:

        !FindInMap[map-name, top-level-key, second-level-key]
        !FindInMap[RegionMap, us-east-1, HVM64]

        Mappings:
            RegionMap:
                us-east-1:
                    HVM64: "ami-12"
                    HVMG2: "ami-13"


    CLOUDFORMATION - OUTPUT:

        NOTE:   When using nesting, this output can be accessible from parent stack.
                Can be exported, allowing cross stack reference.

        Outputs:
            Output-name:
                Description: some description
                value: !Ref some-name 


    CLOUDFORMATION - CONDITIONS:

        Conditions:
            isProd: !Equals         (Note: This is Fn::Equals)
                -   !Ref Env
                -   'prod'

        Type: 'AWS::EC2::Instance'
            Condition: isProd


    DEPENDS ON:

        Cloudformation creates resources in parallel unless implicit dependancy.
        If any depency between resources, can use depends on another resources.

        MYIP:
            TYPE: AWS:EC2:IP
            DependsOn: Another-Resource-Name Another-Resource-Name-2 
            Properties: 


    WAIT CONDITION, CREATION POLICY & CFN-SIGNALS:

        1. Configure cloudformation to hold further process.
        2. wait for x number of success signals (cfn-signal used).
        3. Wait for timeout of H:M:S for those signals (default 12 hrs)
        4. If success signal received, CREATE_COMPLETE
        5. If failure signal received or timeout, CREATION_FAILED

        CreationPolicy is used for EC2 and ASG.
        WaitCondition is used for other AWS resources.

        Creation policy vs wait condition:

            Creation policy is defined in existing resources.
            Wait condition itself resource. 
            Wait condition resource depends upon handle waitconditionhandle.
            AWS recommends to use creationpolicy.


    NESTED STACKS: 

        Stack has limit of 500 resources.
        Cant easily reuse resources.
        Everything under stacks shares same life cycle.

        TYPE: AWS::CloudFromation::Stack
        Properties:
            TemplateURL: https://template-url 
            Parameters: 
                param1: value       (To pass value to template)

        Only can reference output.
        NOTE: Reusing the template.
        have limit of 2500 resources.


    CROSS STACK REFERENCE:

        stacks are separate and attributes are referenced by using export and importvalue.

        Outputs:
            outputname:
                Descrption: 
                Value: 
                Export:
                    Name: export-name 
            
        Fn::ImportValue export-name     -   to reference in another stack.

        NOTE: It should be in same region per account. Cross region is not supoprted.
        NOTE: Reuse the stack.


    STACK SETS: 

        It allows to create/update/delete across many regions and many accouts.

        Concurrent account - how many accounts to process at a time.
        Failure tolerance - amount of individual deployment fail allowed.
        Retain stacks - default delete stack instances and this will retain in target accounts.


    DELETION POLICY:

        Delete      default 
        Retain      not delete 
        Snapshot    suported resources like rds to take snapshot.

    
    STACK ROLES:

        Uses the permission of user identity who run the stack.
        Need permission for both Cloudformation + AWS resources to interact.

        It consists of,
            1. Create a service role which allow to access AWS resources.
            2. Create a pass role for the service role and attach to user.

        
    CFN INIT: 

        It is alternative way to pass user data to ec2 instances.
        AW::Cloudformation::Init - part of native cloud formation.
        Userdata is procedural execution. CF is desired state system which is idempotent.
        cfn-init helper script installed on ec2.

        Metadata:
            AWS:CloudFormation::Init:
                configSets: 
                install_cfn: 
                software_install: 
                configure_instance: 
                    packages:
                    groups:
                    users:
                    groups:
                    sources:
                    files:
                    commands:
                    services: 

        cfn-init helper script executed from user data.
        

    CFN-HUP:

    While update stack, if already created , bootstrap cannot rerun.

        1. cfn-init runs once as part of bootstrapping.
        2. If cloudformation::Init is updated, it cannot rerun themselves.
        3. cfn-hup helper script is used to detect changes in metadata and rerun the cfnint.
        4. cfn hup is added as part of bootstrap.


    CHANGE SETS:

        It helps to preview the changes before apply, 

        It consists of, 
            No interruption
            Some interruption
            Replacement 

    
    CUSTOM RESOURCE: 

        Cloudformation doesnot support everything.
        It is type of template which is used to create template which cloudformation doesnot support.

        Sends event details to Lambda or SNS.

        CustomReousrce:
            Type: 'Custom:name'
            properties:
                ServiceToken: ARNofLambda

        It consists of, 
            1. Write lambda function with custom resource logic.
            2. Deploy lambda function.
            3. Use custom resource in stack.


=========================================================================================================
DEPLOYMENT AND MANAGEMENT: 
=========================================================================================================

NOTE: Refer the aws devops professional preparation for detailed explanation.

SERVICE CATALOG:

    Document or database created by IT team.
    Organised collection of products.
    Offered by IT team.

    It gives self service portal for end users.
    End user permissions are controlled.

    It consists of product and group of product is portfolio. 
    Product is AWS cloudformation template.

    Admins define products and give permission for end user.

    NOTE: For enduser or customer to deploy application with tight control.


CODE COMMIT:

    It is repository like git for AWS
    Support notification and trigger.


CODE PIPELINE:

    Orchastrate flow from repository to deployment.
    Input and output artifacts.
    Manual or auto deployment.


CODE BUILD:

    It is Build as a service.
    Pay only for the resource consumed during build.
    Uses docker (standard or ccustom) to build environments.
    Easy integration with other AWS services.
    customized by using buildspec.yml file in root directory.
    Logs capture in s3 and cloudwatch.
    Eventbridge can be used.

    Different phases,
        install     -   install packages
        pre_build   -   dependancy or login
        build       -   commands run
        post_build  -   packaging, docker push 
        
        environment variables is used.
        Artifacts - Input and output is generate to store in s3.


CODE DEPLOY:

    Deployment as service
    Deploys code not resources.
    Deploys on Ec2, onpremise, lambda and ECS.
    NOTE: Codedeploy agent (Onpremise or ec2).

    File        -   EC2/Onprem
    Resources   -   ECS/Lambda
    Permission  -   EC2/Onprem

    Different hooks are, 
        ApplicationStop
        DownloadBundle
        BeforeInstall
        Install
        AfterInstall
        ApplicationStart
        ValidateService

---------------------------------------------------------------------------------------------------------

OPSWORKS:

    It is a configuration management service with chef and puppet.
    3 modes,
        puppet enterprise - aws managed puppet master service
        chef automate - aws managed chef server 
        opsworks - aws implementation of chef.

    Opsworks components,
        stack - container
        layer - specific function within a stack
        recepie and cookbooks are applied to layer.
        cookbooks are collection of recepie.

    Lifecycle events - setup, configure, deploy, undepoy, shutdown.  Configure run on all instances of stack whenever isntances are added or removed. 

    Instances - 24/7, Load based, Time based. 

---------------------------------------------------------------------------------------------------------

ELASTIC BEANSTALK:

    It is platform as a service.
    It is developer focussed product.
    User provide code and EB handles infrastructure.
    Focus on code and low infrastructure ahead.
    Requires app charges.

    Single container docker, Multi container docker (uses ECS behind)
    Preconfigured docker - used for non natively support platform.
    Packer is used to create a own custom platform.

    1. Create Application by selecting platform and upload code.

    Application versions are used.
    Environment - webserver tier & worker tier. 
    Webserver receive from ELB. Worker receive from SQS. Worker tier can scale based on queue message. 

    CNAME Swap can exchange between two environments.
    Databases keep out of EB. If EB is deleted, database also gets deleted.

    
    DEPLOYMENT POLICIES:

        How application versions are deployed to environment.

        All at once     -   have outage 
        Rolling         -   deploy in rolling batches
        Rolling with additional batch
        Immutable       -   Temporary ASG is created. New instances moved to original ASG and deleted temp ASG.
        Traffic splitting   -   new asg is created. control distribution of percentage of traffic flow between old and new. like A/B testing.
        blue green deployment - 2 different environment and using route 53 route to switch using DNS record.

    
    EBS & RDS:

        1. RDS create directly within EB environment:

                RDS linked to EB
                Delete environment  = delete RDS

        2. RDS create outside of EB:

        Decouple existing rds within EB: 
                1. Take a snapshot of RDS
                2  Enable delete protection
                3. create a new eb environment 
                4. swap environment
                5. terminate old environment 


    ADVANCED CUSTOMIZATION VIA .EBEXTENSIONS FOLDER:

        .ebextensions/.config are way to customise eb environment.
        Uses CFN format to create additional resources within environment.
        
        Resources - create new resources
        option_settings - allows to set of options of resources.
        

    ELASTIC BEANSTALK & HTTPS:

        Apply SSL cert to load balancer directly.
        In Console -> Environment -> load balancer configuration.
        Using .ebextensions/securelistener-alb/nlb.config 

    EB CLONING:

        It allows to create new environment by cloning existing one. 
        Copies options, settings, environment variables, resources.
        Include RDS if within EB.

    EB AND DOCKER:

        Docker running on 64bit Amazon Linux 2:
            uses ec2 and running single container.
            Dockerfile - upload directly to source code. 
            Dockerrun.aws.json (version1) 
            Docker-compose.yml 

        ECS running on 64bit Amazon Linux 2 
            Multiple container application
            creates a ECS cluster.
            Dockerrun.aws.json (version2) file in application source bundle.
            Any images stored in ecr. 

---------------------------------------------------------------------------------------------------------

SYSTEMS MANAGER: 

    Formerly Simple Systems Manager (SSM)
    View and control AWS and onprem infrastructure.
    Agent based which install on windows and Linux based AMI.
    Manage inventory & Patch Assets
    Can define maintenance windows.
    Run commands & manage desired state.
    Parameter store
    Secrets manager
    Session manager


    RUN COMMAND: 

        Run command documents on managed instances.
        No SSH/RDP access required.
        Based on instances, tags, resource group.
        Command documents can be reused and parametrized.
        Error Threshold & Concurrency.


    PATCH MANAGER:

        Patch baseline -    predefined or custom one. it tells what to be installed.
            AWS-OSDefaultPatchBaseline 
            AWS-DefaultPatchBaseine - Windows 
            AWS-WindowsPredefinedPatchBaseline-OS
            AWS-WindowsPredefinedPatchBaseline-OS-Application
            
        Patch group         -   defines group of resources.
        Maintenance window  -   time on which patch occurs.
        Run command         -   to manage the patch process
        concurrency & error threshold
        compliance          - after patch process, checks expected vs applied patch.

        patch baseline -> patch group (resource) -> create maintenance window -> AWS-RunPatchBaseline -> compliance


=========================================================================================================
ADVANCED SECURITY & CONFIG MANAGEMENT: 
=========================================================================================================

AMAZON GUARD DUTY: 

    It is continuos security monitoring system.
    Once enabled continuosly monitoring security issues in account and resources.
    Analyses supported data sources.
    uses AI/ML and threat intelligence feeds.
    Identifies unexpected and unauthorised activity.

    It learns pattern of normal activity and detect if any unpattern activities.
    Notify or event driven remediation.
    Supports multiple accounts (master and member accounts)


AWS CONFIG:

    Record configuration changes overtime on resources.
    Auditing of changes and compliance with standards.
    Doesnot prvent changes from happening. 

    It is a regional service.
    Can configure for cross regions and cross account aggregation.

    Standard functionality:
        1. Enable AWS config and it starts recording configuration changes.
        2. COnfiguration changes are stored in s3 bucket.

    Config rules - Rules are aws or custom rule. Resources are evaluated against rules to check compliant or not. 

    It can integrate with SSM for compliant or non complaint specifically for ec2 instance.

    Rules can have also auto remediation. Trigger when non complaint using SSM Automations. 
    

AWS INSPECTOR:

    It is used to scan ec2 instances and their OS.
    Checks for vulnerabilities and deviations against best practice.
    It generate security report.

    Network Assessment (agentless)
    Network & Host assessment (host agent)
    Network reachability (agentless)

    Rule packages: 
        CVE (Common vulnerabilities and exposure) package.
        CIS (center for internet security) benchmark.
        Security best practices.

    EC2 / container infrastructure (uses SSM agent) -> Inspector -> Security hub (reportings)


AWS WAF & SHEILD: 

    Shield - Gives protection agains DDOS attack.

    It consists of,
        Shield standard - free and available by default. Use with Cloudfront and route53. Protection against layer 3 & 4.
        Shield Advanced - $3000/pm. Support ec2, global accelrator, cloudfront, route53 etc. DDOS response team and financial insurance. 

    WAF: (WebApplication Firewall): 
        Works on layer7.
        SQL injections, cross site scripting, geo blocks.
        Web ACL integrate with ALB, API gateway , cloudfront.
        Rules are added to webacl. 

        WebACL -> Rule Group -> Rules 
        WebACL Capacity Unit . Complex Rules uses higher WCU. 

        Rule groups,
            Baseline rule groups 
            Usecase specific rule groups
            IP reputation rule groups 
            Bot control managed rule group


AWS CERTIFICATE MANAGER: 

    HTTP - Simple and insecure.
    HTTPS - SSL/TLS layer of security added to HTTP.
    Data is encrypted in transit.

    Certificated provide identity signed by Certificate Authority (CA).
    ACM lets you run a public or private certificate Authority.
    With ACM, can generate or import certificate. If generate, renew automatically. If import, renew manually.
    
    Not all aws services are supported. Support cloudfront & ELB. 
    It is a regional service.
    For global service like Cloudfront uses us-east-1.


AWS PARAMETER STORE:

    Store in key value pair.
    Storage for configuration & secrets.

    3 different types,
        String 
        String List 
        Secure String 

    Heirarchies and Versioning.
    Can store as plaintext & ciphertext using KMS. 
    Public parameters provided by AWS.

    It supports 2 tiers,
    standard - 10,000 parameters, 4kb size, no parameter policies, FREE 
    advanced - 100,000 parameters, 8 kb size, parameter policies available, PAID.


AWS SECRETS MANAGER:

    It designed specifically for secrets like pwd, API etc.
    It is integrated with other application.
    Support automatic rotation of key.
    Natively support Direct integration with AWS services like RDS, Redshift, Document DB.
    Support other databases and services using custom lambda function.
    Control access to resources using resource based policy.
    KMS encryption is mandatory.


PARAMETER STORE VS SECRET MANAGER: 

    Parameter store require cloud watch events to trigger lambda for secret rotation.
    Secret managet have builtin to trigger lambda for secret rotation.


VPC FLOWLOGS:

    NOTE: It only capture package metadata not package content.
    Packet sniffer - used to capture the package content.

    It applies to,
        VPC level
        Subnet level 
        Interface only 

    NOTE: it is not real time.

    Configure to use s3 and cloudwatch logs as destination.

    Flow logs not captures, 
        To and from 169.254.169.254 ip address.
        169.254.169.123 
        Amazon windows license not recorded
        DHCP
        Amazon DNS server 


KEY MANAGEMENT SYSTEM (KMS):

    It is regional and public service. 
        NOTE: Multi Region key can also be created. 

    AWS Managed CMK - AWS Managed CMK support key rotation happen every 3 years and cannot be changed or disabled.
    Customer Managed CMK - Customer Managed CMK support key rotation can enable for 1 year

    NOTE: Imported keys cannot auto rotate and support symmetric only. Automatic rotation is not supported. 

    Create, store and manage keys.
    Handle both symmetric and asymmetric keys. All AWS service integration uses symmetric keys. 
    Cryptographic operations (encrypt, decrypt)
    Keys never leave KMS. It provides FIPS 140-2 L2 security standard.
    
    Key policy is like resource policy. 

    CMK - Customer Master Key:
        Can be used for upto 4kb of data.

    DATA ENCRYPTION KEY (DEK):
        Generate data key works on > 4kb size.
        NOTE: KMS doesnot store DEK. 
        Plain text key is discarded.

    KMS can use HSM as custom key store. 


CLOUD HSM (Hardware Security Module): 

    It is similar to KMS.
    Single tenant HSM.
    If lose HSM, lose data.
    FIPS 140 L3 compliant.
    It is not so integrated with AWS.
    Industry standard API PKCS#11 , Java cryptography Extensions (JCE), Microsoft CryptoNG (CNG) libraries support.
    Supports both symmetric and asymmetric encryption.


CLOUD HSM VS KMS: 

    Tenancy - Multi tenancy vs Single tenancy 


=========================================================================================================
OTHER SERVICES:
=========================================================================================================

AMAZON LEX 

    Text or voice conversation.
    Powers the Alexa service.
    Automatic speech recognizition - Speech to text.
    Natural Language understanding.


AMAZON CONNECT: 

    It is contact centre as a service.
    Integrate with PSTN network globally.
    Support voice, chat and both incoming/outgoing calls.
    Can integrate with other AWS services like S3, Lex etc.
    

AWS REKOGNITION:

    Deep leraning based image and video analysis.
    Can detect objects, face recognition, content etc.
    Per image or per minute video processing.
    Can even analyse live stream of kinesis video stream.


KINESIS VIDEO STREAMS:

    It allows to ingest live video stream of data from producers.
    Producers can be security camera, drones, radar data etc.
    Consumers can access data by frame by frame.
    NOTE: Data cant access directly from S3 or other storage but only using API. 
    Integrates with other services like Rekognition, Lex. 


AWS GLUE:

    It is fully managed ETL serverless service.
    Source Data - S3, RDS, JDBC compatible & Dynamodb, kinesis data stream. kafka etc.
    Data target - S3, RDS, JDBC.

    Crawls data sources and generates AWS glue data catalog.
    Data catalog is metadata.

    Glue jobs - do ETL. 

    NOTE: Serverless, adhoc and cost effective then choose Glue than Datapipeline.


DEVICE FARM: 

    Provides manage web and mobile application testing.
    Test on fleet of REAL browser and devices.
    Use builtin or supported automated testing framework.
    Supports remote connections to devices.

=========================================================================================================
DISASTER RECOVERY AND BUSINESS CONTINUITY: 
=========================================================================================================


TYPES OF DR:

    Disaster recovery or business continuity documented is called business continuty plan.
    It involve cost. 
    Extra resource required like backup or secondary region.
    How long to setup depends upon disaster recovery type. 

    Backup & Restore -> Pilot Light -> Warm standby -> Active/Active multi site.

    BACKUP & RESTORE:
        It is less cost.
        Takes time to recover.

    PILOT LIGHT:
        With pilot light, you run second site. 
        It has only core components already setup. 
        Other components need to setup after disaster.
        Little higher cost and little faster to recovery. 

    WARM STANDBY:
        It is minimized version of full copy of original infrasturcture. 
        Ex: Run smaller instance size. 
        When needed it will scale the infrastructure.
        Faster than pilot light. 

    ACTIVE/ACTIVE MULTISITE:
        Both primary and secondary running parallely.
        If any one site is down traffic is routed to secondary.
        Higher cost and faster recovery.


DR ARCHITECTURE - STORAGE:

    Instance store - Attach to Ec2 in AZ level. No recovery process. 
    EBS - Also attach to EC2 in AZ level. Volumes are replicated within AZ only. Take snapshot of EBS volume. It can potentially tolerate the failure of hardware in AZ. 
    S3 - Data is replicated across AZ. Take snapshot to another region. 
    EFS - one file system is replicated across multi AZ.


DR ARCHITECTURE - COMPUTE:

    It is mostly based on regional.
    EC2 - runs in one AZ. No resilience. 
    ASG - Runs in different AZ. 
    ECS - Use services to configure for high availability.
    Lambda - Can run in any AZ in event of failure.


DR ARCHITECTURE - DATABASE:

    Dynamodb - Runs in public zone. data is replicated in multiple AZ. 
    RDS - Subnet group has primary and standby instances. Synchronous replication occurs. On failure, failover to secondary. RDS multiAZ can tolerate the failure of one AZ in a region.
    Aurora - Can have 1 replica in each AZ. Uses cluster storage architecture. Data is replicated across AZ. 

    Dynamodb - can create global tables in different region. Multi master replication happens. 
    Aurora - Aurora global database. NOTE: Writes only allowed in primary region and secondary regions allows read only. Not multi master replication. Done at storage level. 
    Normal RDS - Cross region read replicas in another region. It is asynchronous level. Not at storage level.


DR ARCHITECTURE - NETWORKING:

    VPC - regional level. 
    Each subnet is within one AZ and cannot span across AZ. 
    Subnet - Fails if AZ is down. 
    ELB - Regional services - ELB -> Instances across AZ. 
    Interface endpoint - tied to specific subnet in one AZ. Adding multiple interface endpoint in different AZ can withstand AZ failure.
    Route 53 - used for global service which act as load balance. 
    ENI cant tolerate the failure of AZ in a region.
    IGW - resilient by design and can tolerate failure of many AZs in a region.
    NAT Gateway - resilient by design and choose the specific subnet so cant tolerate the failure of AZ.


=========================================================================================================
MIGRATIONS & EXTENSIONS:
=========================================================================================================

THE 6R'S OF CLOUD MIGRATION:

    Rehosting   -   Lift and shift:
        VM Import/Export & Server Migration Service 

    Replatform  -   Lift and shift with optimization:
        Use RDS for database.
        It doesnot involve huge changes but use few AWS services.

    Repurchase  -   Move to something new as SAAS:

    Refactoring/Rearchitecting  -   Take advantage of cloud:
        Review and change the architecture to use cloud native.
        Service oriented or micro services.
        APIs, Event driven, serverless.
        Very expensive and time consuming. 
        Best long term benefits. 

    Retire      -   Retire 
    Retain      -   Retain 

---------------------------------------------------------------------------------------------------------

VIRTUAL MACHINE MIGRATIONS:

    Application Discovery Service - Allows to discover onprem applications.
        Agentless   -   Measure performance and resource usage. OVA appliance, integrates with VMWare - inventory, vm usage. 

        Agent       -   Network, process, usage, performance and dependencies between servers (network activity)

        Integrate AWS Migration Hub and Athena. 


    Server Migration Service: 

        Migrate whole Vm's (OS, data, Apps)
        Which performs actual migration.
        Agentless, interact through connector installed in VMware. 
        Works with vmware, hyperv and azure vm only. 
        Incremental replication of live volume 
        Orchestrate multi server migration.
        Creates AMI 
        Integrate with AWS Migration Hub

        Onprem connector -> S3 -> SMS -> AMI -> EC2 instances.

    
    Database Migration service:

        Source and target database.
        Can also use from AWS to AWS or from/to AWS migrations.
        Source can be s3. 
        One endpoint must be on AWS.
        Source and destination endpoint. 
        It uses replication instances which consists of replication tasks.
        not natively support schema conversion.
        Different options are,
            Full load 
            Full load + CDC (ongoing changes)
            CDC only (Change data capture)


    Schema Conversion Tool (SCT):

        Used when converting from one database engine to another.
        Not used when migrating DB of same type.
        On prem mysql -> RDS mysql (Same engine so cannot use SCT)
        On prem MS SQL -> RDS mysql -> Moving from microsoft sql to my sql different engine.
        Target can be s3. 


    Snowball:

        Transform TB of data 

---------------------------------------------------------------------------------------------------------

STORAGE GATEWAY:

    It runs as virual appliance in onprem.
    Act as bridge between onprem storage and AWS 
    In AWS, integrate with EBS, S3 or glacier.  
    In Onprem side, support iSCSI, NFS or SMB. 
    Migrations, Extensions, Storage tiering, DR. 


STORAGE GATEWAY - VOLUME GATEWAY:
    
    Cached mode:
        Data is not located locally in onprem but stored in AWS s3 (Note only AWS manage and user cant see in s3)
        Only cached data stored in onprem.
        As data center extension. 

    Stored Mode: 
        All stored locally. Copy to S3 as ebs snapshot. 
        Great for FULL DISK backup. 
        Excellant RPO and RTO. 
        NOTE: Doesnot allow extend capability since data stored locally and send to AWS. 
        Used for backup.
        It is to have AWS backups and want to maintain low latency access to all data. 

STORAGE GATEWAY - TAPE GATEWAY:

        In AWS, have 2 types,
            VTL (Virtual Tape Library) - backed by s3
            VTS (Virtual Tape Shelf) - backed by glacier - archives from VTL. 
        Present as hardware. 

STORAGE GATEWAY - FILE GATEWAY:

    Bridge onprem file to s3 
    Mounpoints available via NFS or SMB.
    Mountpoint created in file gateway. Files mounted in gateway are visible as objects in s3.
    As data center extension.     

    It involves,
        1. create a file share which integrate with 1 bucket. 
        2. Mapping between filename and object name.
        3. Can integrate with other AWS services from s3.
    NOTE:   Notify when upoaded is used to initiate from S3 to filegateway. 
            Doesnot support object locking.
            Can use s3 lifecycle policies.

---------------------------------------------------------------------------------------------------------

AWS SNOWBALL:

    It is physical device. 
    Data encrypted using KMS. 
    50Tb or 80TB capacity.
    Multiple devices to multiple premises.
    Only storage and doesnot have compute.

SNOWBALL EDGE: 
    Its have both storage and compute.
    Larger capacity 
    It consists of,
        storage optimised with ec2.
        compute optimised 
        compute optimised with gpu capability 

SNOW MOBILE:
    Truck used to move data to AWS.
    Not available everywhere.
    Suitable for 10+ PB data.
    Upto 100 PB of data.
    Not economical for multi site. 

---------------------------------------------------------------------------------------------------------

AWS DATASYNC: 

    It orchastrate the movement of data of large scale from onprem to AWS or vice versa.
    Migrations, data processing transfers, archival, cost effective storage or DR.
    Keeps metadata (timestamp, permissions)
    builtin data validation.
    10gb/s per agent 
    Compression and encryption
    Automatic recovery from transit error 
    AWS service integration.
    Incremental and scheduled transfer options.

    1. Datasync agent install on onprem NFS or SMB protocol.
    2. Agent communicates with AWS.
    3. In AWS, data can store in S3, EFS, FSx

---------------------------------------------------------------------------------------------------------

SSL OFFLOAD USING CLOUD HSM:  

    client -> ELB -> EC2 -> Cloud HSM (Used to store SSL details)


S3 BUCKET POLICY:  

    AWS:SourceIP        -   Used to allow/block from public 
    AWS:SourceVpce / AWS:SourceVpc     -   used to allow/block from gateway endpoint. 



